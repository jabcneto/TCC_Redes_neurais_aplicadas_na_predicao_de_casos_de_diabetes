{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# !pip3 install scikit-learn==1.6.0 pandas numpy matplotlib seaborn xgboost lime tdqm imblearn shap lime",
   "id": "584ad90aa3b138e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#=====================================================================\n",
    "# IMPORTAÇÃO DE BIBLIOTECAS\n",
    "# =====================================================================\n",
    "\n",
    "# Manipulação e análise de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Visualização\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Pré-processamento e splitting\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Métricas de avaliação\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, confusion_matrix,\n",
    "    f1_score, precision_score, recall_score, roc_auc_score, roc_curve, auc,\n",
    "    precision_recall_curve, average_precision_score, classification_report\n",
    ")\n",
    "\n",
    "# Modelos clássicos\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Modelos avançados (boosting)\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Redes neurais (Keras/TensorFlow)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Activation, PReLU, LeakyReLU, Dense, Dropout,\n",
    "    Conv1D, MaxPooling1D, Flatten, BatchNormalization,\n",
    "    LSTM, Bidirectional, GlobalAveragePooling1D\n",
    ")\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau,\n",
    "    TensorBoard, CSVLogger\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Balanceamento de dados\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Interpretabilidade\n",
    "import shap\n",
    "from lime import lime_tabular"
   ],
   "id": "2440bd5a92574909",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =====================================================================\n",
    "# CONFIGURAÇÕES GERAIS\n",
    "# =====================================================================\n",
    "logging.basicConfig(\n",
    "    level=logging.ERROR,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"diabetes_prediction.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Criar diretório para salvar resultados\n",
    "RESULTS_DIR = \"resultados_diabetes\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{RESULTS_DIR}/modelos\", exist_ok=True)\n",
    "os.makedirs(f\"{RESULTS_DIR}/graficos/confusao\", exist_ok=True)\n",
    "os.makedirs(f\"{RESULTS_DIR}/graficos/roc\", exist_ok=True)\n",
    "os.makedirs(f\"{RESULTS_DIR}/graficos/importancia\", exist_ok=True)\n",
    "os.makedirs(f\"{RESULTS_DIR}/graficos/distribuicao\", exist_ok=True)\n",
    "os.makedirs(f\"{RESULTS_DIR}/graficos/shap\", exist_ok=True)\n",
    "os.makedirs(f\"{RESULTS_DIR}/graficos/lime\", exist_ok=True)\n",
    "os.makedirs(f\"{RESULTS_DIR}/logs\", exist_ok=True)\n",
    "os.makedirs(f\"{RESULTS_DIR}/history\", exist_ok=True)"
   ],
   "id": "d72f0d25ad005192",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =====================================================================\n",
    "# FUNÇÕES DE CARREGAMENTO E PRÉ-PROCESSAMENTO DE DADOS\n",
    "# =====================================================================\n",
    "\n",
    "# Dicionário de mapeamento das colunas para português\n",
    "MAPEAMENTO_COLUNAS_PT = {\n",
    "    'gender': 'gênero',\n",
    "    'age': 'idade',\n",
    "    'hypertension': 'hipertensão',\n",
    "    'heart_disease': 'doença cardíaca',\n",
    "    'smoking_history': 'histórico de tabagismo',\n",
    "    'bmi': 'IMC',\n",
    "    'HbA1c_level': 'nível de HbA1c',\n",
    "    'blood_glucose_level': 'nível de glicose no sangue',\n",
    "    'diabetes': 'diabetes'\n",
    "}\n",
    "\n",
    "# Função para renomear as colunas do DataFrame para português\n",
    "def renomear_colunas_para_portugues(df):\n",
    "    return df.rename(columns=MAPEAMENTO_COLUNAS_PT)\n",
    "\n",
    "def carregar_dados(caminho_arquivo, verbose=True):\n",
    "    \"\"\"\n",
    "    Carrega o dataset de predição de diabete e realiza análise exploratória inicial.\n",
    "\n",
    "    Args:\n",
    "        caminho_arquivo (str): Caminho para o arquivo CSV do dataset\n",
    "        verbose (bool): Se True, exibe informações sobre o dataset\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame com os dados carregados\n",
    "    \"\"\"\n",
    "    logger.info(f\"Carregando dados de {caminho_arquivo}\")\n",
    "\n",
    "    try:\n",
    "        dataframe = pd.read_csv(caminho_arquivo)\n",
    "        dataframe = renomear_colunas_para_portugues(dataframe)\n",
    "        if verbose:\n",
    "            logger.info(f\"Dataset carregado com sucesso. Formato: {dataframe.shape}\")\n",
    "            logger.info(f\"Colunas: {dataframe.columns.tolist()}\")\n",
    "            logger.info(f\"Tipos de dados:\\n{dataframe.dtypes}\")\n",
    "            logger.info(\n",
    "                f\"Distribuição da variável alvo (diabetes):\\n{dataframe['diabetes'].value_counts(normalize=True) * 100}\")\n",
    "\n",
    "        return dataframe\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao carregar o dataset: {e}\")\n",
    "\n",
    "\n",
    "def analisar_dados(df):\n",
    "    \"\"\"\n",
    "    Realiza análise exploratória detalhada dos dados.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame com os dados\n",
    "\n",
    "    Returns:\n",
    "        dict: Dicionário com estatísticas e informações da análise\n",
    "    \"\"\"\n",
    "    logger.info(\"Realizando análise exploratória dos dados\")\n",
    "\n",
    "    # Estatísticas básicas\n",
    "    estatisticas = {\n",
    "        'shape': df.shape,\n",
    "        'missing_values': df.isnull().sum().to_dict(),\n",
    "        'dtypes': df.dtypes.to_dict(),\n",
    "        'target_distribution': df['diabetes'].value_counts(normalize=True).to_dict(),\n",
    "        'numeric_stats': df.describe().to_dict(),\n",
    "    }\n",
    "\n",
    "    # Análise de variáveis categóricas\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    estatisticas['categorical_counts'] = {col: df[col].value_counts().to_dict() for col in cat_cols}\n",
    "\n",
    "    # Deteção de outliers (usando IQR)\n",
    "    num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    outliers = {}\n",
    "    for col in num_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers[col] = {\n",
    "            'lower_bound': lower_bound,\n",
    "            'upper_bound': upper_bound,\n",
    "            'n_outliers': ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
    "        }\n",
    "    estatisticas['outliers'] = outliers\n",
    "\n",
    "    # Correlações\n",
    "    # Garante que apenas colunas numéricas sejam usadas para correlação\n",
    "    numeric_df = df.select_dtypes(include=np.number)\n",
    "    if 'diabetes' in numeric_df.columns:\n",
    "        estatisticas['correlations'] = numeric_df.corr()['diabetes'].to_dict()\n",
    "    else:\n",
    "        estatisticas['correlations'] = {}\n",
    "        logger.warning(\"Coluna 'diabetes' não encontrada ou não é numérica para cálculo de correlação.\")\n",
    "\n",
    "    # Visualizações\n",
    "    visualizar_analise_exploratoria(df)\n",
    "\n",
    "    return estatisticas\n",
    "\n",
    "\n",
    "def visualizar_analise_exploratoria(df):\n",
    "    \"\"\"\n",
    "    Cria visualizações para análise exploratória dos dados.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): DataFrame com os dados\n",
    "    \"\"\"\n",
    "    logger.info(\"Gerando visualizações para análise exploratória\")\n",
    "    # Configuração para visualizações\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    sns.set_palette('viridis')\n",
    "\n",
    "    # 1. Distribuição da variável alvo\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.countplot(x='diabetes', data=df, palette=['#3498db', '#e74c3c'], hue='diabetes', legend=False)\n",
    "    plt.title('Distribuição da Variável Alvo (Diabetes)', fontsize=15)\n",
    "    plt.xlabel('Diabetes', fontsize=12)\n",
    "    plt.ylabel('Contagem', fontsize=12)\n",
    "\n",
    "    # Adicionar percentagens\n",
    "    total = len(df)\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x() + p.get_width() / 2., height + 5,\n",
    "                f'{height} ({height / total:.1%})',\n",
    "                ha=\"center\", fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/distribuicao/distribuicao_target.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Distribuição das variáveis numéricas por status de diabete\n",
    "    num_cols = ['idade', 'IMC', 'nível de HbA1c', 'nível de glicose no sangue']\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    for i, col in enumerate(num_cols):\n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        sns.histplot(data=df, x=col, hue='diabetes', kde=True, bins=30,\n",
    "                     palette=['#3498db', '#e74c3c'], alpha=0.6)\n",
    "        plt.title(f'Distribuição de {col} por Status de Diabetes', fontsize=13)\n",
    "        plt.xlabel(col, fontsize=11)\n",
    "        plt.ylabel('Contagem', fontsize=11)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/distribuicao/distribuicao_variaveis_numericas.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 3. Boxplots para variáveis numéricas\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    for i, col in enumerate(num_cols):\n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        sns.boxplot(x='diabetes', y=col, data=df, palette=['#3498db', '#e74c3c'], hue='diabetes', legend=False)\n",
    "        plt.title(f'{col} por Status de Diabetes', fontsize=13)\n",
    "        plt.xlabel('Diabetes', fontsize=11)\n",
    "        plt.ylabel(col, fontsize=11)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/boxplots_variaveis_numericas.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 4. Matriz de correlação\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    # Seleciona apenas colunas numéricas para a matriz de correlação\n",
    "    num_df = df.select_dtypes(include=[np.number])\n",
    "    corr_matrix = num_df.corr()\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "                square=True, linewidths=0.5)\n",
    "    plt.title('Matriz de Correlação', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/matriz_correlacao.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 5. Pairplot para variáveis numéricas\n",
    "    # Garante que só as colunas numéricas e a coluna alvo sejam usadas\n",
    "    pairplot_cols = [col for col in num_cols if col in num_df.columns] + ['diabetes']\n",
    "    sns.pairplot(df[pairplot_cols], hue='diabetes',\n",
    "                 palette=['#3498db', '#e74c3c'], diag_kind='kde')\n",
    "    plt.suptitle('Pairplot de Variáveis Numéricas', y=1.02, fontsize=16)\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/pairplot_variaveis_numericas.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 6. Contagem de variáveis categóricas\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    if cat_cols:\n",
    "        plt.figure(figsize=(15, 5 * len(cat_cols)))\n",
    "\n",
    "        for i, col in enumerate(cat_cols):\n",
    "            plt.subplot(len(cat_cols), 1, i + 1)\n",
    "            sns.countplot(x=col, hue='diabetes', data=df, palette=['#3498db', '#e74c3c'], legend=False)\n",
    "            plt.title(f'Distribuição de {col} por Status de Diabetes', fontsize=13)\n",
    "            plt.xlabel(col, fontsize=11)\n",
    "            plt.ylabel('Contagem', fontsize=11)\n",
    "            plt.xticks(rotation=45)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{RESULTS_DIR}/graficos/distribuicao/distribuicao_variaveis_categoricas.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    # 7. Relação entre HbA1c e glicose com diabete\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    scatter = sns.scatterplot(data=df, x='nível de HbA1c', y='nível de glicose no sangue',\n",
    "                              hue='diabetes', palette=['#3498db', '#e74c3c'],\n",
    "                              s=80, alpha=0.7)\n",
    "    plt.axvline(x=6.5, color='red', linestyle='--', label='Limiar HbA1c (6.5%)')\n",
    "    plt.axhline(y=126, color='green', linestyle='--', label='Limiar Glicose (126 mg/dL)')\n",
    "\n",
    "    # Adicionar anotações para os quadrantes\n",
    "    plt.text(7.5, 200, 'Alto risco\\n(HbA1c alto, Glicose alta)', fontsize=12, ha='center')\n",
    "    plt.text(5.5, 200, 'Risco moderado\\n(HbA1c normal, Glicose alta)', fontsize=12, ha='center')\n",
    "    plt.text(7.5, 100, 'Risco moderado\\n(HbA1c alto, Glicose normal)', fontsize=12, ha='center')\n",
    "    plt.text(5.5, 100, 'Baixo risco\\n(HbA1c normal, Glicose normal)', fontsize=12, ha='center')\n",
    "\n",
    "    plt.title('Relação entre HbA1c e Glicose no Sangue', fontsize=15)\n",
    "    plt.xlabel('Nível de HbA1c (%)', fontsize=12)\n",
    "    plt.ylabel('Nível de Glicose (mg/dL)', fontsize=12)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/relacao_hba1c_glicose.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def pre_processar_dados(dataframe, metodo_normalizacao='standard',\n",
    "                        metodo_balanceamento='smote',\n",
    "                        tratar_outliers=True,\n",
    "                        metodo_outliers='iqr',\n",
    "                        fator_outliers=1.5,\n",
    "                        percentis_outliers=(5, 95),\n",
    "                        test_size=0.2,\n",
    "                        val_size=0.15,\n",
    "                        random_state=42):\n",
    "    \"\"\"\n",
    "    Realiza o pré-processamento completo dos dados para treinamento do modelo.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): DataFrame com os dados\n",
    "        metodo_normalizacao (str): Método de normalização ('standard', 'minmax', 'robust')\n",
    "        metodo_balanceamento (str): Método de balanceamento ('smote', 'adasyn', 'borderline', 'tomek', 'none')\n",
    "        tratar_outliers (bool): Se True, trata outliers\n",
    "        metodo_outliers (str): Método para tratar outliers ('iqr', 'zscore', 'percentil', 'isolation_forest', 'winsorization')\n",
    "        fator_outliers (float): Fator multiplicativo para IQR ou threshold para Z-score\n",
    "        percentis_outliers (tuple): Percentis inferior e superior para método percentil\n",
    "        test_size (float): Proporção do conjunto de teste\n",
    "        val_size (float): Proporção do conjunto de validação (em relação ao conjunto não-teste)\n",
    "        random_state (int): Semente aleatória\n",
    "\n",
    "    Returns:\n",
    "        tuple: (x_train, x_val, x_test, y_train, y_val, y_test, preprocessador, feature_names)\n",
    "    \"\"\"\n",
    "    logger.info(\"Iniciando pré-processamento dos dados\")\n",
    "\n",
    "    # Remover valores 'Other' da coluna gender (se existir)\n",
    "    if 'gender' in dataframe.columns and 'Other' in dataframe['gender'].unique():\n",
    "        logger.info(\"Removendo valores 'Other' da coluna gender\")\n",
    "        dataframe = dataframe[dataframe['gender'] != 'Other']\n",
    "\n",
    "    # Separação entre atributos e rótulo\n",
    "    x = dataframe.drop(columns=['diabetes']).copy()\n",
    "    y = dataframe['diabetes'].copy()\n",
    "\n",
    "    # Identificar colunas numéricas e categóricas\n",
    "    colunas_numericas = x.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    colunas_categoricas = x.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    logger.info(f\"Colunas numéricas: {colunas_numericas}\")\n",
    "    logger.info(f\"Colunas categóricas: {colunas_categoricas}\")\n",
    "\n",
    "    # Armazenar nomes das features antes da codificação\n",
    "    feature_names_original = x.columns.tolist()\n",
    "\n",
    "    # Codificar variáveis categóricas\n",
    "    encoder = None\n",
    "    if colunas_categoricas:\n",
    "        logger.info(\"Codificando variáveis categóricas usando One-Hot Encoding\")\n",
    "        encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "\n",
    "        # Aplicar one-hot encoding\n",
    "        categorical_encoded = encoder.fit_transform(x[colunas_categoricas])\n",
    "\n",
    "        # Obter nomes das novas colunas\n",
    "        categorical_feature_names = encoder.get_feature_names_out(colunas_categoricas)\n",
    "\n",
    "        # Criar DataFrame com dados codificados\n",
    "        categorical_df = pd.DataFrame(categorical_encoded,\n",
    "                                    columns=categorical_feature_names,\n",
    "                                    index=x.index)\n",
    "\n",
    "        # Combinar dados numéricos com categóricos codificados\n",
    "        x_processed = pd.concat([x[colunas_numericas], categorical_df], axis=1)\n",
    "\n",
    "        # Atualizar lista de nomes das features\n",
    "        feature_names = colunas_numericas + categorical_feature_names.tolist()\n",
    "\n",
    "        logger.info(f\"Variáveis categóricas codificadas. Novas features: {categorical_feature_names.tolist()}\")\n",
    "    else:\n",
    "        x_processed = x.copy()\n",
    "        feature_names = feature_names_original\n",
    "\n",
    "    # Atualizar lista de colunas numéricas (agora inclui todas as colunas)\n",
    "    colunas_numericas_final = x_processed.columns.tolist()\n",
    "\n",
    "    # Tratamento de outliers\n",
    "    if tratar_outliers:\n",
    "        logger.info(f\"Tratando outliers usando método '{metodo_outliers}'\")\n",
    "        x_processed = tratar_outliers_avancado(\n",
    "            x_processed, colunas_numericas_final,\n",
    "            metodo=metodo_outliers,\n",
    "            fator=fator_outliers,\n",
    "            percentis=percentis_outliers\n",
    "        )\n",
    "\n",
    "    # Normalização\n",
    "    scaler = None\n",
    "    if metodo_normalizacao == 'standard':\n",
    "        logger.info(\"Aplicando normalização padrão (StandardScaler)\")\n",
    "        scaler = StandardScaler()\n",
    "        x_processed[colunas_numericas_final] = scaler.fit_transform(x_processed[colunas_numericas_final])\n",
    "    elif metodo_normalizacao == 'minmax':\n",
    "        logger.info(\"Aplicando normalização Min-Max (MinMaxScaler)\")\n",
    "        scaler = MinMaxScaler()\n",
    "        x_processed[colunas_numericas_final] = scaler.fit_transform(x_processed[colunas_numericas_final])\n",
    "    elif metodo_normalizacao == 'robust':\n",
    "        logger.info(\"Aplicando normalização robusta (RobustScaler)\")\n",
    "        scaler = RobustScaler()\n",
    "        x_processed[colunas_numericas_final] = scaler.fit_transform(x_processed[colunas_numericas_final])\n",
    "    else:\n",
    "        logger.warning(f\"Método de normalização '{metodo_normalizacao}' não reconhecido. Usando dados originais.\")\n",
    "\n",
    "    # Converter para numpy array para algoritmos de balanceamento\n",
    "    x_array = x_processed.values\n",
    "    y_array = y.values\n",
    "\n",
    "    # Balanceamento de classes\n",
    "    if metodo_balanceamento == 'smote':\n",
    "        logger.info(\"Aplicando SMOTE para balanceamento de classes\")\n",
    "        smote = SMOTE(random_state=random_state)\n",
    "        x_array, y_array = smote.fit_resample(x_array, y_array)\n",
    "    elif metodo_balanceamento == 'adasyn':\n",
    "        logger.info(\"Aplicando ADASYN para balanceamento de classes\")\n",
    "        adasyn = ADASYN(random_state=random_state)\n",
    "        x_array, y_array = adasyn.fit_resample(x_array, y_array)\n",
    "    elif metodo_balanceamento == 'borderline':\n",
    "        logger.info(\"Aplicando BorderlineSMOTE para balanceamento de classes\")\n",
    "        borderline = BorderlineSMOTE(random_state=random_state)\n",
    "        x_array, y_array = borderline.fit_resample(x_array, y_array)\n",
    "    elif metodo_balanceamento == 'tomek':\n",
    "        logger.info(\"Aplicando SMOTETomek para balanceamento de classes\")\n",
    "        tomek = SMOTETomek(random_state=random_state)\n",
    "        x_array, y_array = tomek.fit_resample(x_array, y_array)\n",
    "    elif metodo_balanceamento == 'none':\n",
    "        logger.info(\"Nenhum balanceamento aplicado\")\n",
    "    else:\n",
    "        logger.warning(f\"Método de balanceamento '{metodo_balanceamento}' não reconhecido. Usando dados originais.\")\n",
    "\n",
    "    logger.info(f\"Dados após balanceamento: {x_array.shape}, distribuição das classes: {np.bincount(y_array.astype(int))}\")\n",
    "\n",
    "    # Divisão em conjuntos de treino, validação e teste\n",
    "    x_train, x_temp, y_train, y_temp = train_test_split(\n",
    "        x_array, y_array, test_size=test_size, random_state=random_state, stratify=y_array\n",
    "    )\n",
    "    x_val, x_test, y_val, y_test = train_test_split(\n",
    "        x_temp, y_temp, test_size=val_size / (test_size), random_state=random_state, stratify=y_temp\n",
    "    )\n",
    "\n",
    "    # Salvar pré-processador\n",
    "    preprocessador = {\n",
    "        'scaler': scaler,\n",
    "        'encoder': encoder,\n",
    "        'feature_selection': None,  # Placeholder para futura seleção de features\n",
    "        'feature_names_original': feature_names_original,\n",
    "        'feature_names_final': feature_names\n",
    "    }\n",
    "\n",
    "    logger.info(f\"Divisão dos dados concluída:\")\n",
    "    logger.info(f\"  Treino: {x_train.shape}\")\n",
    "    logger.info(f\"  Validação: {x_val.shape}\")\n",
    "    logger.info(f\"  Teste: {x_test.shape}\")\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test, preprocessador, feature_names\n",
    "\n",
    "\n",
    "def tratar_outliers_avancado(df, colunas_numericas, metodo='iqr', fator=1.5, percentis=(5, 95)):\n",
    "    \"\"\"\n",
    "    Trata outliers de forma mais eficiente usando diferentes métodos.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame com os dados\n",
    "        colunas_numericas (list): Lista de colunas numéricas\n",
    "        metodo (str): Método para tratar outliers ('iqr', 'zscore', 'percentil', 'isolation_forest')\n",
    "        fator (float): Fator multiplicativo para IQR ou threshold para Z-score\n",
    "        percentis (tuple): Percentis inferior e superior para método percentil\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame com outliers tratados\n",
    "    \"\"\"\n",
    "    df_tratado = df.copy()\n",
    "\n",
    "    if metodo == 'iqr':\n",
    "        logger.info(f\"Tratando outliers usando IQR com fator {fator}\")\n",
    "        # Calcular todos os quantis de uma vez (operação vetorizada)\n",
    "        q1 = df_tratado[colunas_numericas].quantile(0.25)\n",
    "        q3 = df_tratado[colunas_numericas].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bounds = q1 - fator * iqr\n",
    "        upper_bounds = q3 + fator * iqr\n",
    "\n",
    "        # Aplicar capping vetorizado\n",
    "        df_tratado[colunas_numericas] = df_tratado[colunas_numericas].clip(\n",
    "            lower=lower_bounds, upper=upper_bounds, axis=1\n",
    "        )\n",
    "\n",
    "    elif metodo == 'zscore':\n",
    "        logger.info(f\"Tratando outliers usando Z-score com threshold {fator}\")\n",
    "        from scipy import stats\n",
    "\n",
    "        # Calcular Z-scores para todas as colunas\n",
    "        z_scores = np.abs(stats.zscore(df_tratado[colunas_numericas]))\n",
    "\n",
    "        # Criar máscara para outliers\n",
    "        outlier_mask = (z_scores > fator).any(axis=1)\n",
    "        logger.info(f\"Identificados {outlier_mask.sum()} outliers com Z-score > {fator}\")\n",
    "\n",
    "        # Substituir outliers pela mediana da coluna\n",
    "        for col in colunas_numericas:\n",
    "            col_z_scores = np.abs(stats.zscore(df_tratado[col]))\n",
    "            outliers = col_z_scores > fator\n",
    "            mediana = df_tratado[col].median()\n",
    "            df_tratado.loc[outliers, col] = mediana\n",
    "\n",
    "    elif metodo == 'percentil':\n",
    "        logger.info(f\"Tratando outliers usando percentis {percentis}\")\n",
    "        # Calcular percentis para todas as colunas\n",
    "        lower_bounds = df_tratado[colunas_numericas].quantile(percentis[0]/100)\n",
    "        upper_bounds = df_tratado[colunas_numericas].quantile(percentis[1]/100)\n",
    "\n",
    "        # Aplicar capping vetorizado\n",
    "        df_tratado[colunas_numericas] = df_tratado[colunas_numericas].clip(\n",
    "            lower=lower_bounds, upper=upper_bounds, axis=1\n",
    "        )\n",
    "\n",
    "    elif metodo == 'isolation_forest':\n",
    "        logger.info(\"Tratando outliers usando Isolation Forest\")\n",
    "        from sklearn.ensemble import IsolationForest\n",
    "\n",
    "        # Treinar Isolation Forest\n",
    "        iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "        outlier_pred = iso_forest.fit_predict(df_tratado[colunas_numericas])\n",
    "\n",
    "        # Identificar outliers (-1 indica outlier)\n",
    "        outliers = outlier_pred == -1\n",
    "        logger.info(f\"Identificados {outliers.sum()} outliers com Isolation Forest\")\n",
    "\n",
    "        # Substituir outliers pela mediana\n",
    "        for col in colunas_numericas:\n",
    "            mediana = df_tratado[col].median()\n",
    "            df_tratado.loc[outliers, col] = mediana\n",
    "\n",
    "    elif metodo == 'winsorization':\n",
    "        logger.info(f\"Tratando outliers usando Winsorização com percentis {percentis}\")\n",
    "        from scipy.stats.mstats import winsorize\n",
    "\n",
    "        # Aplicar winsorização para cada coluna\n",
    "        for col in colunas_numericas:\n",
    "            df_tratado[col] = winsorize(\n",
    "                df_tratado[col],\n",
    "                limits=((100-percentis[1])/100, (100-percentis[0])/100)\n",
    "            )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Método '{metodo}' não reconhecido. Use 'iqr', 'zscore', 'percentil', 'isolation_forest' ou 'winsorization'\")\n",
    "\n",
    "    # Calcular estatísticas de outliers removidos\n",
    "    outliers_removidos = {}\n",
    "    for col in colunas_numericas:\n",
    "        valores_originais = df[col]\n",
    "        valores_tratados = df_tratado[col]\n",
    "        modificados = (valores_originais != valores_tratados).sum()\n",
    "        outliers_removidos[col] = {\n",
    "            'modificados': modificados,\n",
    "            'percentual': (modificados / len(df)) * 100\n",
    "        }\n",
    "\n",
    "    logger.info(\"Resumo do tratamento de outliers:\")\n",
    "    for col, stats in outliers_removidos.items():\n",
    "        logger.info(f\"{col}: {stats['modificados']} valores modificados ({stats['percentual']:.2f}%)\")\n",
    "\n",
    "    return df_tratado"
   ],
   "id": "169a4c5e7493d5c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =====================================================================\n",
    "# FUNÇÕES PARA CRIAÇÃO DE MODELOS\n",
    "# =====================================================================\n",
    "\n",
    "def criar_modelo_mlp(input_shape,\n",
    "                     camadas=[128, 64, 32],\n",
    "                     activations=['relu', 'relu', 'relu'],\n",
    "                     dropout_rates=[0.3, 0.3, 0.3],\n",
    "                     regularization=0.001,\n",
    "                     learning_rate=0.005,\n",
    "                     batch_norm=True):\n",
    "    \"\"\"\n",
    "    Cria um modelo MLP (Perceptron Multicamadas) para classificação binária.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Forma dos dados de entradas\n",
    "        camadas (list): Lista com o número de neurônios em cada camada oculta\n",
    "        activations (list): Lista com as funções de ativação para cada camada\n",
    "        dropout_rates (list): Lista com as taxas de dropout para cada camada\n",
    "        regularization (float): Coeficiente de regularização L2\n",
    "        learning_rate (float): Taxa de aprendizado\n",
    "        batch_norm (bool): Se True, adiciona camadas de Batch Normalization\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: Modelo MLP compilado\n",
    "    \"\"\"\n",
    "    logger.info(f\"Criando modelo MLP com {len(camadas)} camadas ocultas\")\n",
    "    logger.info(f\"Arquitetura: {camadas}\")\n",
    "    logger.info(f\"Ativações: {activations}\")\n",
    "    logger.info(f\"Dropout rates: {dropout_rates}\")\n",
    "\n",
    "    # Verificar se os parâmetros têm o mesmo comprimento\n",
    "    assert len(camadas) == len(activations) == len(dropout_rates), \\\n",
    "        \"camadas, ativacoes e dropout_rates devem ter o mesmo comprimento\"\n",
    "\n",
    "    # Criar modelo\n",
    "    model = Sequential()\n",
    "\n",
    "    # Camada de entrada\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    # Camadas ocultas\n",
    "    for i, (units, activation, dropout_rate) in enumerate(zip(camadas, activations, dropout_rates)):\n",
    "        # Adicionar camada densa com regularização L2\n",
    "        model.add(Dense(\n",
    "            units=units,\n",
    "            kernel_regularizer=l2(regularization),\n",
    "            name=f'dense_{i + 1}'\n",
    "        ))\n",
    "\n",
    "        # Adicionar batch normalization antes da ativação\n",
    "        if batch_norm:\n",
    "            model.add(BatchNormalization(name=f'batch_norm_{i + 1}'))\n",
    "\n",
    "        # Adicionar função de ativação\n",
    "        if activation == 'leaky_relu':\n",
    "            model.add(LeakyReLU(alpha=0.1, name=f'leaky_relu_{i + 1}'))\n",
    "        elif activation == 'prelu':\n",
    "            model.add(PReLU(name=f'prelu_{i + 1}'))\n",
    "        else:\n",
    "            model.add(Activation(activation, name=f'activation_{i + 1}'))\n",
    "\n",
    "        # Adicionar dropout para regularização\n",
    "        if dropout_rate > 0:\n",
    "            model.add(Dropout(dropout_rate, name=f'dropout_{i + 1}'))\n",
    "\n",
    "    # Camada de saída\n",
    "    model.add(Dense(1, activation='sigmoid', name='output'))\n",
    "\n",
    "    # Compilar modelo\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            'recall'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Resumo do modelo\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def criar_modelo_cnn(input_shape,\n",
    "                     filtros=[64, 32, 16],\n",
    "                     kernel_sizes=[3, 3, 3],\n",
    "                     ativacoes=['relu', 'relu', 'relu'],\n",
    "                     dropout_rates=[0.3, 0.3, 0.3],\n",
    "                     pool_sizes=[2, 2, 2],\n",
    "                     regularizacao=0.001,\n",
    "                     learning_rate=0.001,\n",
    "                     batch_norm=True):\n",
    "    \"\"\"\n",
    "    Cria um modelo CNN 1D para classificação binária de dados tabulares.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Forma dos dados de entrada (deve ser 3D para CNN)\n",
    "        filtros (list): Lista com o número de filtros em cada camada convolucional\n",
    "        kernel_sizes (list): Lista com os tamanhos do kernel para cada camada\n",
    "        ativacoes (list): Lista com as funções de ativação para cada camada\n",
    "        dropout_rates (list): Lista com as taxas de dropout para cada camada\n",
    "        pool_sizes (list): Lista com os tamanhos de pooling para cada camada\n",
    "        regularizacao (float): Coeficiente de regularização L2\n",
    "        learning_rate (float): Taxa de aprendizado\n",
    "        batch_norm (bool): Se True, adiciona camadas de Batch Normalization\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: Modelo CNN compilado\n",
    "    \"\"\"\n",
    "    logger.info(f\"Criando modelo CNN com {len(filtros)} camadas convolucionais\")\n",
    "    logger.info(f\"Filtros: {filtros}\")\n",
    "    logger.info(f\"Kernel sizes: {kernel_sizes}\")\n",
    "    logger.info(f\"Ativações: {ativacoes}\")\n",
    "\n",
    "    # Verificar se os parâmetros têm o mesmo comprimento\n",
    "    assert len(filtros) == len(kernel_sizes) == len(ativacoes) == len(dropout_rates) == len(pool_sizes), \\\n",
    "        \"filtros, kernel_sizes, ativacoes, dropout_rates e pool_sizes devem ter o mesmo comprimento\"\n",
    "\n",
    "    # Verificar se a forma de entrada é 3D\n",
    "    if len(input_shape) != 2:\n",
    "        logger.warning(f\"Input shape {input_shape} não é 3D. Adicionando dimensão extra.\")\n",
    "        input_shape = (*input_shape, 1)\n",
    "\n",
    "    # Criar modelo\n",
    "    model = Sequential()\n",
    "\n",
    "    # Camada de entrada\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    # Camadas convolucionais\n",
    "    for i, (filters, kernel_size, activation, dropout_rate, pool_size) in enumerate(\n",
    "            zip(filtros, kernel_sizes, ativacoes, dropout_rates, pool_sizes)):\n",
    "\n",
    "        # Adicionar camada convolucional com regularização L2\n",
    "        model.add(Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            padding='same',\n",
    "            kernel_regularizer=l2(regularizacao),\n",
    "            name=f'conv_{i + 1}'\n",
    "        ))\n",
    "\n",
    "        # Adicionar batch normalization antes da ativação\n",
    "        if batch_norm:\n",
    "            model.add(BatchNormalization(name=f'batch_norm_{i + 1}'))\n",
    "\n",
    "        # Adicionar função de ativação\n",
    "        if activation == 'leaky_relu':\n",
    "            model.add(LeakyReLU(alpha=0.1, name=f'leaky_relu_{i + 1}'))\n",
    "        elif activation == 'prelu':\n",
    "            model.add(PReLU(name=f'prelu_{i + 1}'))\n",
    "        else:\n",
    "            model.add(Activation(activation, name=f'activation_{i + 1}'))\n",
    "\n",
    "        # Adicionar max pooling\n",
    "        model.add(MaxPooling1D(pool_size=pool_size, padding='same', name=f'pool_{i + 1}'))\n",
    "\n",
    "        # Adicionar dropout para regularização\n",
    "        if dropout_rate > 0:\n",
    "            model.add(Dropout(dropout_rate, name=f'dropout_{i + 1}'))\n",
    "\n",
    "    # Flatten para conectar às camadas densas\n",
    "    model.add(Flatten(name='flatten'))\n",
    "\n",
    "    # Camada densa intermediária\n",
    "    model.add(Dense(32, kernel_regularizer=l2(regularizacao), name='dense_1'))\n",
    "    if batch_norm:\n",
    "        model.add(BatchNormalization(name='batch_norm_dense'))\n",
    "    model.add(Activation('relu', name='activation_dense'))\n",
    "    model.add(Dropout(0.3, name='dropout_dense'))\n",
    "\n",
    "    # Camada de saída\n",
    "    model.add(Dense(1, activation='sigmoid', name='output'))\n",
    "\n",
    "    # Compilar modelo\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            'recall'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Resumo do modelo\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def criar_modelo_hibrido(input_shape,\n",
    "                         filtros_cnn=[64, 32],\n",
    "                         kernel_sizes=[3, 3],\n",
    "                         unidades_lstm=32,\n",
    "                         unidades_densas=[64, 32],\n",
    "                         ativacoes=['relu', 'relu'],\n",
    "                         dropout_rates=[0.3, 0.3],\n",
    "                         regularizacao=0.001,\n",
    "                         learning_rate=0.001,\n",
    "                         batch_norm=True):\n",
    "    \"\"\"\n",
    "    Cria um modelo híbrido CNN-LSTM para classificação binária de dados tabulares.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Forma dos dados de entrada (deve ser 3D)\n",
    "        filtros_cnn (list): Lista com o número de filtros em cada camada convolucional\n",
    "        kernel_sizes (list): Lista com os tamanhos do kernel para cada camada\n",
    "        unidades_lstm (int): Número de unidades na camada LSTM\n",
    "        unidades_densas (list): Lista com o número de neurônios em cada camada densa\n",
    "        ativacoes (list): Lista com as funções de ativação para cada camada densa\n",
    "        dropout_rates (list): Lista com as taxas de dropout para cada camada densa\n",
    "        regularizacao (float): Coeficiente de regularização L2\n",
    "        learning_rate (float): Taxa de aprendizado\n",
    "        batch_norm (bool): Se True, adiciona camadas de Batch Normalization\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: Modelo híbrido CNN-LSTM compilado\n",
    "    \"\"\"\n",
    "    logger.info(\"Criando modelo híbrido CNN-LSTM\")\n",
    "    logger.info(f\"Filtros CNN: {filtros_cnn}\")\n",
    "    logger.info(f\"Unidades LSTM: {unidades_lstm}\")\n",
    "    logger.info(f\"Unidades densas: {unidades_densas}\")\n",
    "\n",
    "    # Verificar se a forma de entrada é 3D\n",
    "    if len(input_shape) != 2:\n",
    "        logger.warning(f\"Input shape {input_shape} não é 3D. Adicionando dimensão extra.\")\n",
    "        input_shape = (*input_shape, 1)\n",
    "\n",
    "    # Verificar se os parâmetros têm o mesmo comprimento\n",
    "    assert len(filtros_cnn) == len(kernel_sizes), \\\n",
    "        \"filtros_cnn e kernel_sizes devem ter o mesmo comprimento\"\n",
    "    assert len(unidades_densas) == len(ativacoes) == len(dropout_rates), \\\n",
    "        \"unidades_densas, ativacoes e dropout_rates devem ter o mesmo comprimento\"\n",
    "\n",
    "    # Criar modelo usando a API funcional\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Camadas convolucionais\n",
    "    x = inputs\n",
    "    for i, (filters, kernel_size) in enumerate(zip(filtros_cnn, kernel_sizes)):\n",
    "        x = Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            padding='same',\n",
    "            kernel_regularizer=l2(regularizacao),\n",
    "            name=f'conv_{i + 1}'\n",
    "        )(x)\n",
    "\n",
    "        if batch_norm:\n",
    "            x = BatchNormalization(name=f'batch_norm_conv_{i + 1}')(x)\n",
    "\n",
    "        x = Activation('relu', name=f'activation_conv_{i + 1}')(x)\n",
    "        x = MaxPooling1D(pool_size=2, padding='same', name=f'pool_{i + 1}')(x)\n",
    "\n",
    "    # Camada LSTM bidirecional\n",
    "    x = Bidirectional(LSTM(unidades_lstm, return_sequences=True, name='lstm_1'))(x)\n",
    "    x = Dropout(0.3, name='dropout_lstm')(x)\n",
    "\n",
    "    # Global Average Pooling\n",
    "    x = GlobalAveragePooling1D(name='global_avg_pool')(x)\n",
    "\n",
    "    # Camadas densas\n",
    "    for i, (units, activation, dropout_rate) in enumerate(zip(unidades_densas, ativacoes, dropout_rates)):\n",
    "        x = Dense(units, kernel_regularizer=l2(regularizacao), name=f'dense_{i + 1}')(x)\n",
    "\n",
    "        if batch_norm:\n",
    "            x = BatchNormalization(name=f'batch_norm_dense_{i + 1}')(x)\n",
    "\n",
    "        if activation == 'leaky_relu':\n",
    "            x = LeakyReLU(alpha=0.1, name=f'leaky_relu_{i + 1}')(x)\n",
    "        elif activation == 'prelu':\n",
    "            x = PReLU(name=f'prelu_{i + 1}')(x)\n",
    "        else:\n",
    "            x = Activation(activation, name=f'activation_dense_{i + 1}')(x)\n",
    "\n",
    "        if dropout_rate > 0:\n",
    "            x = Dropout(dropout_rate, name=f'dropout_dense_{i + 1}')(x)\n",
    "\n",
    "    # Camada de saída\n",
    "    outputs = Dense(1, activation='sigmoid', name='output')(x)\n",
    "\n",
    "    # Criar e compilar modelo\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            'recall'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Resumo do modelo\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def criar_callbacks(nome_modelo, paciencia=10, min_delta=0.001, fator_reducao=0.5, min_lr=1e-6):\n",
    "    \"\"\"\n",
    "    Cria callbacks para treinamento do modelo.\n",
    "\n",
    "    Args:\n",
    "        nome_modelo (str): Nome do modelo para salvar\n",
    "        paciencia (int): Número de épocas para esperar antes de parar o treinamento\n",
    "        min_delta (float): Mínima mudança para considerar como melhoria\n",
    "        fator_reducao (float): Fator para reduzir a taxa de aprendizado\n",
    "        min_lr (float): Taxa de aprendizado mínima\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de callbacks\n",
    "    \"\"\"\n",
    "    # Criar diretório para ‘logs’ do TensorBoard\n",
    "    log_dir = f\"{RESULTS_DIR}/logs/{nome_modelo}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        # Early stopping para evitar overfitting\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=paciencia,\n",
    "            min_delta=min_delta,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "\n",
    "        # Model checkpoint para salvar o melhor modelo\n",
    "        ModelCheckpoint(\n",
    "            filepath=f\"{RESULTS_DIR}/modelos/{nome_modelo}_best.h5\",\n",
    "            monitor='val_auc',\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "\n",
    "        # Reduce learning rate quando o treinamento estagnar\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=fator_reducao,\n",
    "            patience=paciencia // 2,\n",
    "            min_lr=min_lr,\n",
    "            verbose=1\n",
    "        ),\n",
    "\n",
    "        # TensorBoard para visualização do treino\n",
    "        TensorBoard(\n",
    "            log_dir=log_dir,\n",
    "            histogram_freq=1,\n",
    "            write_graph=True,\n",
    "            update_freq='epoch'\n",
    "        ),\n",
    "\n",
    "        # CSV Logger para salvar histórico de treino\n",
    "        CSVLogger(\n",
    "            filename=f\"{RESULTS_DIR}/history/{nome_modelo}_history.csv\",\n",
    "            separator=',',\n",
    "            append=False\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return callbacks"
   ],
   "id": "d4f2ceb2634d7b79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =====================================================================\n",
    "# FUNÇÕES PARA COMPARAÇÃO DE MODELOS\n",
    "# =====================================================================\n",
    "\n",
    "def treinar_modelos_classicos(x_train, y_train, x_test, y_test, feature_names):\n",
    "    \"\"\"\n",
    "    Treina e avalia modelos clássicos de machine learning.\n",
    "\n",
    "    Args:\n",
    "        x_train (np.ndarray): Dados de treinamento\n",
    "        y_train (np.ndarray): Rótulos de treinamento\n",
    "        x_test (np.ndarray): Dados de teste\n",
    "        y_test (np.ndarray): Rótulos de teste\n",
    "        feature_names (list): Nomes das features\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame com resultados dos modelos\n",
    "    \"\"\"\n",
    "    logger.info(\"Treinando modelos clássicos de machine learning\")\n",
    "\n",
    "    # Garantir que x_train e x_test sejam DataFrames com nomes de features\n",
    "    if not isinstance(x_train, pd.DataFrame):\n",
    "        x_train = pd.DataFrame(x_train, columns=feature_names)\n",
    "    if not isinstance(x_test, pd.DataFrame):\n",
    "        x_test = pd.DataFrame(x_test, columns=feature_names)\n",
    "\n",
    "    # Definir modelos\n",
    "    modelos = {\n",
    "        'Regressão Logística': LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced', verbose=1),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', verbose=1),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42, verbose=1),\n",
    "        'XGBoost': XGBClassifier(eval_metric='logloss', use_label_encoder=False, random_state=42, verbose=1, n_jobs=-1),\n",
    "        'LightGBM': LGBMClassifier(random_state=42, verbose=1, n_jobs=-1),\n",
    "        'SVM': SVC(kernel='rbf', probability=True, random_state=42, class_weight='balanced', verbose=1),\n",
    "        'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n",
    "        'Naive Bayes': GaussianNB(),\n",
    "    }\n",
    "\n",
    "    # Resultados\n",
    "    resultados = []\n",
    "\n",
    "    # Treinar e avaliar cada modelo\n",
    "    for nome, modelo in modelos.items():\n",
    "        logger.info(f\"Treinando modelo: {nome}\")\n",
    "\n",
    "        # Treinar modelo\n",
    "        modelo.fit(x_train, y_train)\n",
    "\n",
    "        # Fazer predições\n",
    "        y_pred = modelo.predict(x_test)\n",
    "\n",
    "        # Obter probabilidades\n",
    "        if hasattr(modelo, \"predict_proba\"):\n",
    "            y_prob = modelo.predict_proba(x_test)[:, 1]\n",
    "        else:\n",
    "            # Para SVM sem probabilidades\n",
    "            y_prob = modelo.decision_function(x_test)\n",
    "            # Normalizar para [0, 1]\n",
    "            y_prob = (y_prob - y_prob.min()) / (y_prob.max() - y_prob.min())\n",
    "\n",
    "        # Calcular métricas\n",
    "        metrics = {\n",
    "            'modelo': nome,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_prob),\n",
    "            'average_precision': average_precision_score(y_test, y_prob)\n",
    "        }\n",
    "\n",
    "        resultados.append(metrics)\n",
    "\n",
    "        # Salvar modelo\n",
    "        with open(f\"{RESULTS_DIR}/modelos/modelo_{nome.replace(' ', '_').lower()}.pkl\", 'wb') as f:\n",
    "            pickle.dump(modelo, f)\n",
    "\n",
    "        # Visualizar matriz de confusão\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "        plt.title(f'Matriz de Confusão - {nome}', fontsize=15)\n",
    "        plt.ylabel('Valor Real', fontsize=12)\n",
    "        plt.xlabel('Valor Predito', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{RESULTS_DIR}/graficos/confusao/matriz_confusao_{nome.replace(' ', '_').lower()}.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # Visualizar curva ROC\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Taxa de Falsos Positivos', fontsize=12)\n",
    "        plt.ylabel('Taxa de Verdadeiros Positivos', fontsize=12)\n",
    "        plt.title(f'Curva ROC - {nome}', fontsize=15)\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{RESULTS_DIR}/graficos/roc/curva_roc_{nome.replace(' ', '_').lower()}.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # Para Random Forest, visualizar importância das features\n",
    "        if nome == 'Random Forest':\n",
    "            importances = modelo.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1]\n",
    "\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.bar(range(len(indices[:15])), importances[indices[:15]], align='center')\n",
    "            plt.xticks(range(len(indices[:15])), [feature_names[i] for i in indices[:15]], rotation=90)\n",
    "            plt.title('Importância das Features - Random Forest', fontsize=15)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{RESULTS_DIR}/graficos/importancia/importancia_features_random_forest.png\", dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "    # Criar DataFrame com resultados\n",
    "    resultados_df = pd.DataFrame(resultados)\n",
    "\n",
    "    # Salvar resultados\n",
    "    resultados_df.to_csv(f\"{RESULTS_DIR}/resultados_modelos_classicos.csv\", index=False)\n",
    "\n",
    "    return resultados_df\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# FUNÇÕES PARA TREINAMENTO E AVALIAÇÃO DE MODELOS\n",
    "# =====================================================================\n",
    "\n",
    "def treinar_modelo(modelo, x_train, y_train, x_val, y_val,\n",
    "                   nome_modelo, batch_size=32, epochs=100,\n",
    "                   class_weight=None, verbose=1):\n",
    "    \"\"\"\n",
    "    Treina o modelo de rede neural.\n",
    "\n",
    "    Args:\n",
    "        modelo (tf.keras.Model): Modelo a ser treinado\n",
    "        x_train (np.ndarray): Dados de treinos\n",
    "        y_train (np.ndarray): Rótulos de treinos\n",
    "        x_val (np.ndarray): Dados de validação\n",
    "        y_val (np.ndarray): Rótulos de validação\n",
    "        nome_modelo (str): Nome do modelo para salvar\n",
    "        batch_size (int): Tamanho do batch\n",
    "        epochs (int): Número máximo de épocas\n",
    "        class_weight (dict): Pesos para as classes\n",
    "        verbose (int): Nível de verbosidade\n",
    "\n",
    "    Returns:\n",
    "        tuple: (modelo treinado, histórico de treinos)\n",
    "    \"\"\"\n",
    "    logger.info(f\"Iniciando treinamento do modelo {nome_modelo}\")\n",
    "    logger.info(f\"Batch size: {batch_size}, Épocas: {epochs}\")\n",
    "\n",
    "    # Verificar se é necessário reshape para CNN ou modelos híbridos\n",
    "    if 'cnn' in nome_modelo.lower() or 'hibrido' in nome_modelo.lower():\n",
    "        if len(x_train.shape) == 2:\n",
    "            logger.info(\"Adicionando dimensão extra para dados CNN/híbridos\")\n",
    "            x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "            x_val = x_val.reshape(x_val.shape[0], x_val.shape[1], 1)\n",
    "\n",
    "    # Calcular class weights se não fornecidos\n",
    "    if class_weight is None and len(np.unique(y_train)) == 2:\n",
    "        logger.info(\"Calculando pesos de classe para balanceamento\")\n",
    "        n_samples = len(y_train)\n",
    "        n_positives = np.sum(y_train)\n",
    "        n_negatives = n_samples - n_positives\n",
    "\n",
    "        weight_for_0 = (1 / n_negatives) * (n_samples / 2.0)\n",
    "        weight_for_1 = (1 / n_positives) * (n_samples / 2.0)\n",
    "\n",
    "        class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "        logger.info(f\"Class weights: {class_weight}\")\n",
    "\n",
    "    # Criar callbacks\n",
    "    callbacks = criar_callbacks(nome_modelo)\n",
    "\n",
    "    # Registrar tempo de início\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Treinar o modelo\n",
    "    history = modelo.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        class_weight=class_weight,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Calcular tempo de treinamento\n",
    "    training_time = time.time() - start_time\n",
    "    logger.info(f\"Treinamento concluído em {training_time:.2f} segundos\")\n",
    "\n",
    "    # Carregar o melhor modelo\n",
    "    try:\n",
    "        logger.info(f\"Carregando o melhor modelo de {RESULTS_DIR}/modelos/{nome_modelo}_best.h5\")\n",
    "        modelo = load_model(f\"{RESULTS_DIR}/modelos/{nome_modelo}_best.h5\")\n",
    "    except:\n",
    "        logger.warning(\"Não foi possível carregar o melhor modelo. Usando o modelo atual.\")\n",
    "\n",
    "    # Salvar o modelo final\n",
    "    modelo.save(f\"{RESULTS_DIR}/modelos/{nome_modelo}_final.h5\")\n",
    "    logger.info(f\"Modelo final salvo em {RESULTS_DIR}/modelos/{nome_modelo}_final.h5\")\n",
    "\n",
    "    # Salvar histórico de treino\n",
    "    hist_df = pd.DataFrame(history.history)\n",
    "    hist_df.to_csv(f\"{RESULTS_DIR}/history/{nome_modelo}_history_full.csv\", index=False)\n",
    "\n",
    "\n",
    "def avaliar_modelo(modelo, x_test, y_test, nome_modelo, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Avalia o desempenho do modelo nos dados de teste.\n",
    "\n",
    "    Args:\n",
    "        modelo (tf.keras.Model): Modelo treinado\n",
    "        x_test (np.ndarray): Dados de teste\n",
    "        y_test (np.ndarray): Rótulos de teste\n",
    "        nome_modelo (str): Nome do modelo para salvar resultados\n",
    "        threshold (float): Limiar para classificação binária\n",
    "\n",
    "    Returns:\n",
    "        dict: Dicionário com métricas de desempenho\n",
    "    \"\"\"\n",
    "    logger.info(f\"Avaliando modelo {nome_modelo} nos dados de teste\")\n",
    "\n",
    "    # Verificar se é necessário reshape para CNN ou modelos híbridos\n",
    "    if 'cnn' in nome_modelo.lower() or 'hibrido' in nome_modelo.lower():\n",
    "        if len(x_test.shape) == 2:\n",
    "            logger.info(\"Adicionando dimensão extra para dados CNN/híbridos\")\n",
    "            x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)\n",
    "\n",
    "    # Fazer predições\n",
    "    y_pred_prob = modelo.predict(x_test)\n",
    "    y_pred = (y_pred_prob > threshold).astype(int).flatten()\n",
    "    y_pred_prob = y_pred_prob.flatten()\n",
    "\n",
    "    # Calcular métricas\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_prob),\n",
    "        'average_precision': average_precision_score(y_test, y_pred_prob)\n",
    "    }\n",
    "\n",
    "    # Exibir métricas\n",
    "    logger.info(\"Métricas de desempenho:\")\n",
    "    for metric, value in metrics.items():\n",
    "        logger.info(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    # Salvar métricas em CSV\n",
    "    pd.DataFrame([metrics]).to_csv(f\"{RESULTS_DIR}/{nome_modelo}_metricas.csv\", index=False)\n",
    "\n",
    "    # Matriz de confusão\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Relatório de classificação\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    pd.DataFrame(report).transpose().to_csv(f\"{RESULTS_DIR}/{nome_modelo}_classification_report.csv\")\n",
    "\n",
    "    # Visualizações\n",
    "    visualizar_resultados(y_test, y_pred, y_pred_prob, nome_modelo)\n",
    "\n",
    "    return metrics, y_pred, y_pred_prob\n",
    "\n",
    "\n",
    "def visualizar_resultados(y_true, y_pred, y_prob, nome_modelo):\n",
    "    \"\"\"\n",
    "    Cria visualizações para os resultados do modelo.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): Rótulos verdadeiros\n",
    "        y_pred (np.ndarray): Predições binárias\n",
    "        y_prob (np.ndarray): Probabilidades preditas\n",
    "        nome_modelo (str): Nome do modelo para salvar visualizações\n",
    "    \"\"\"\n",
    "    logger.info(f\"Gerando visualizações para resultados do modelo {nome_modelo}\")\n",
    "\n",
    "    # 1. Matriz de confusão\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Normalizar matriz de confusão\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Plotar matriz de confusão\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title(f'Matriz de Confusão - {nome_modelo}', fontsize=15)\n",
    "    plt.ylabel('Valor Real', fontsize=12)\n",
    "    plt.xlabel('Valor Predito', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/{nome_modelo}_matriz_confusao.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Matriz de confusão normalizada\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', cbar=False)\n",
    "    plt.title(f'Matriz de Confusão Normalizada - {nome_modelo}', fontsize=15)\n",
    "    plt.ylabel('Valor Real', fontsize=12)\n",
    "    plt.xlabel('Valor Predito', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/{nome_modelo}_matriz_confusao_norm.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Curva ROC\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Taxa de Falsos Positivos', fontsize=12)\n",
    "    plt.ylabel('Taxa de Verdadeiros Positivos', fontsize=12)\n",
    "    plt.title(f'Curva ROC - {nome_modelo}', fontsize=15)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/{nome_modelo}_curva_roc.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 3. Curva Precision-Recall\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "    avg_precision = average_precision_score(y_true, y_prob)\n",
    "\n",
    "    plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AP = {avg_precision:.4f})')\n",
    "    plt.axhline(y=sum(y_true) / len(y_true), color='red', linestyle='--', label='Baseline')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall', fontsize=12)\n",
    "    plt.ylabel('Precision', fontsize=12)\n",
    "    plt.title(f'Curva Precision-Recall - {nome_modelo}', fontsize=15)\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/{nome_modelo}_curva_precision_recall.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 4. Histograma de probabilidades\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Separar probabilidades por classe\n",
    "    prob_pos = y_prob[y_true == 1]\n",
    "    prob_neg = y_prob[y_true == 0]\n",
    "\n",
    "    plt.hist(prob_pos, bins=20, alpha=0.5, color='green', label='Classe Positiva (Diabetes)')\n",
    "    plt.hist(prob_neg, bins=20, alpha=0.5, color='red', label='Classe Negativa (Não Diabetes)')\n",
    "\n",
    "    plt.axvline(x=0.5, color='black', linestyle='--', label='Limiar (0.5)')\n",
    "    plt.xlabel('Probabilidade Predita', fontsize=12)\n",
    "    plt.ylabel('Contagem', fontsize=12)\n",
    "    plt.title(f'Distribuição de Probabilidades - {nome_modelo}', fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/{nome_modelo}_distribuicao_probabilidades.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def visualizar_historico_treinamento(historico, nome_modelo):\n",
    "    \"\"\"\n",
    "    Visualiza o histórico de treinamento do modelo.\n",
    "\n",
    "    Args:\n",
    "        historico (tf.keras.callbacks.History): Histórico de treinamento\n",
    "        nome_modelo (str): Nome do modelo para salvar visualizações\n",
    "    \"\"\"\n",
    "    logger.info(f\"Visualizando histórico de treinamento do modelo {nome_modelo}\")\n",
    "\n",
    "    # 1. Curvas de perda (loss)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(historico['loss'], label='Treino', color='blue')\n",
    "    plt.plot(historico['val_loss'], label='Validação', color='orange')\n",
    "    plt.title(f'Curvas de Perda - {nome_modelo}', fontsize=15)\n",
    "    plt.xlabel('Época', fontsize=12)\n",
    "    plt.ylabel('Perda (Binary Crossentropy)', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/{nome_modelo}_curvas_perda.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Curvas de acurácia\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(historico['accuracy'], label='Treino', color='blue')\n",
    "    plt.plot(historico['val_accuracy'], label='Validação', color='orange')\n",
    "    plt.title(f'Curvas de Acurácia - {nome_modelo}', fontsize=15)\n",
    "    plt.xlabel('Época', fontsize=12)\n",
    "    plt.ylabel('Acurácia', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/{nome_modelo}_curvas_acuracia.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 3. Curvas de métricas adicionais\n",
    "    if 'precision' in historico.columns and 'recall' in historico.columns and 'auc' in historico.columns:\n",
    "        plt.figure(figsize=(18, 6))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(historico['precision'], label='Treino', color='blue')\n",
    "        plt.plot(historico['val_precision'], label='Validação', color='orange')\n",
    "        plt.title('Precisão', fontsize=13)\n",
    "        plt.xlabel('Época', fontsize=11)\n",
    "        plt.ylabel('Precisão', fontsize=11)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(historico['recall'], label='Treino', color='blue')\n",
    "        plt.plot(historico['val_recall'], label='Validação', color='orange')\n",
    "        plt.title('Recall', fontsize=13)\n",
    "        plt.xlabel('Época', fontsize=11)\n",
    "        plt.ylabel('Recall', fontsize=11)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(historico['auc'], label='Treino', color='blue')\n",
    "        plt.plot(historico['val_auc'], label='Validação', color='orange')\n",
    "        plt.title('AUC', fontsize=13)\n",
    "        plt.xlabel('Época', fontsize=11)\n",
    "        plt.ylabel('AUC', fontsize=11)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.suptitle(f'Métricas de Treinamento - {nome_modelo}', fontsize=16, y=1.05)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{RESULTS_DIR}/graficos/{nome_modelo}_curvas_metricas.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    # 4. Curvas de aprendizado (Learning Curves)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Subplot para loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(historico['loss'], label='Treino', color='blue')\n",
    "    plt.plot(historico['val_loss'], label='Validação', color='orange')\n",
    "    plt.title('Perda (Loss)', fontsize=13)\n",
    "    plt.xlabel('Época', fontsize=11)\n",
    "    plt.ylabel('Perda', fontsize=11)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Subplot para accuracy\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(historico['accuracy'], label='Treino', color='blue')\n",
    "    plt.plot(historico['val_accuracy'], label='Validação', color='orange')\n",
    "    plt.title('Acurácia', fontsize=13)\n",
    "    plt.xlabel('Época', fontsize=11)\n",
    "    plt.ylabel('Acurácia', fontsize=11)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Calcular diferença entre treino e validação\n",
    "    if 'loss' in historico.columns and 'val_loss' in historico.columns:\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(historico['loss'] - historico['val_loss'], label='Diferença', color='green')\n",
    "        plt.axhline(y=0, color='red', linestyle='--')\n",
    "        plt.title('Diferença de Perda (Treino - Validação)', fontsize=13)\n",
    "        plt.xlabel('Época', fontsize=11)\n",
    "        plt.ylabel('Diferença', fontsize=11)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Calcular razão entre treino e validação\n",
    "    if 'accuracy' in historico.columns and 'val_accuracy' in historico.columns:\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(historico['accuracy'] / historico['val_accuracy'], label='Razão', color='purple')\n",
    "        plt.axhline(y=1, color='red', linestyle='--')\n",
    "        plt.title('Razão de Acurácia (Treino / Validação)', fontsize=13)\n",
    "        plt.xlabel('Época', fontsize=11)\n",
    "        plt.ylabel('Razão', fontsize=11)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle(f'Curvas de Aprendizado - {nome_modelo}', fontsize=16, y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/{nome_modelo}_curvas_aprendizado.png\", dpi=300)\n",
    "    plt.close()"
   ],
   "id": "de5b54d11727c88d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =====================================================================\n",
    "# FUNÇÕES PARA INTERPRETABILIDADE DE MODELOS\n",
    "# =====================================================================\n",
    "\n",
    "def analisar_importancia_variaveis(modelo, x_test, y_test, feature_names, nome_modelo, n_repeats=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Analisa a importância das variáveis no modelo usando cálculo manual de permutation importance com roc_auc.\n",
    "\n",
    "    Args:\n",
    "        modelo (tf.keras.Model): Modelo treinado\n",
    "        x_test (np.ndarray): Dados de teste\n",
    "        y_test (np.ndarray): Rótulos de teste\n",
    "        feature_names (list): Nomes das features\n",
    "        nome_modelo (str): Nome do modelo para salvar resultados\n",
    "        n_repeats (int): Número de repetições para permutação\n",
    "        random_state (int): Semente aleatória\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame com importância das variáveis\n",
    "    \"\"\"\n",
    "    logger.info(f\"Analisando importância das variáveis para o modelo {nome_modelo} (cálculo manual)\")\n",
    "\n",
    "    # Determina se o modelo é CNN ou híbrido com base no nome\n",
    "    is_cnn_or_hybrid = 'cnn' in nome_modelo.lower() or 'hibrido' in nome_modelo.lower()\n",
    "\n",
    "    # Garante que x_test seja 2D para a lógica de permutação\n",
    "    x_test_2d = x_test\n",
    "    if is_cnn_or_hybrid and len(x_test.shape) == 3:\n",
    "        logger.info(\"Convertendo dados 3D para 2D para análise de importância\")\n",
    "        x_test_2d = x_test.reshape(x_test.shape[0], x_test.shape[1])\n",
    "\n",
    "    # Função para obter probabilidades do modelo Keras (considerando reshape)\n",
    "    def get_proba(x_input):\n",
    "        x_input_internal = x_input  # Usa cópia para evitar modificar original\n",
    "        if is_cnn_or_hybrid and len(x_input_internal.shape) == 2:\n",
    "            x_input_internal = x_input_internal.reshape(x_input_internal.shape[0], x_input_internal.shape[1], 1)\n",
    "        return modelo.predict(x_input_internal)[:, 0]  # Retorna prob da classe 1\n",
    "\n",
    "    # Calcula a pontuação base (AUC)\n",
    "    try:\n",
    "        baseline_proba = get_proba(x_test_2d)\n",
    "        baseline_score = roc_auc_score(y_test, baseline_proba)\n",
    "        logger.info(f\"Baseline ROC AUC score: {baseline_score:.4f}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao calcular baseline score para {nome_modelo}: {e}\")\n",
    "        return pd.DataFrame()  # Retorna DataFrame vazio em caso de falha\n",
    "\n",
    "    # Inicializa arrays para armazenar importâncias\n",
    "    importances_mean = np.zeros(x_test_2d.shape[1])\n",
    "    importances_std = np.zeros(x_test_2d.shape[1])\n",
    "    all_perm_scores = [[] for _ in range(x_test_2d.shape[1])]\n",
    "\n",
    "    # Itera sobre cada feature\n",
    "    logger.info(\"Calculando importância por permutação manual (pode levar algum tempo)\")\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    for col_idx in tqdm(range(x_test_2d.shape[1]), desc=f\"Permutando features - {nome_modelo}\"):\n",
    "        original_col = x_test_2d[:, col_idx].copy()\n",
    "        perm_scores = []\n",
    "\n",
    "        for _ in range(n_repeats):\n",
    "            # Permuta a coluna atual\n",
    "            x_test_permuted = x_test_2d.copy()\n",
    "            x_test_permuted[:, col_idx] = rng.permutation(original_col)\n",
    "\n",
    "            # Calcula a pontuação com a coluna permutada\n",
    "            try:\n",
    "                permuted_proba = get_proba(x_test_permuted)\n",
    "                perm_score = roc_auc_score(y_test, permuted_proba)\n",
    "                perm_scores.append(perm_score)\n",
    "                all_perm_scores[col_idx].append(perm_score)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Erro ao calcular score para feature {col_idx} permutada: {e}\")\n",
    "                perm_scores.append(np.nan)  # Adiciona NaN em caso de erro\n",
    "                all_perm_scores[col_idx].append(np.nan)\n",
    "\n",
    "        # Calcula a importância como a diferença para a baseline\n",
    "        valid_perm_scores = [s for s in perm_scores if not np.isnan(s)]\n",
    "        if valid_perm_scores:\n",
    "            importances_mean[col_idx] = baseline_score - np.mean(valid_perm_scores)\n",
    "            importances_std[col_idx] = np.std(valid_perm_scores)\n",
    "        else:\n",
    "            importances_mean[col_idx] = 0  # Define importância como 0 se todos os cálculos falharam\n",
    "            importances_std[col_idx] = 0\n",
    "\n",
    "    # Organizar resultados\n",
    "    importancia = pd.DataFrame({\n",
    "        'Feature': feature_names[:x_test_2d.shape[1]],  # Garantir que o número de features corresponda\n",
    "        'Importância': importances_mean,\n",
    "        'Desvio Padrão': importances_std\n",
    "    })\n",
    "\n",
    "    importancia = importancia.sort_values('Importância', ascending=False)\n",
    "\n",
    "    # Salvar resultados\n",
    "    importancia.to_csv(f\"{RESULTS_DIR}/{nome_modelo}_importancia_variaveis_manual.csv\", index=False)\n",
    "\n",
    "    # Visualizar importância das variáveis (top 15)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    top_features = importancia.head(15)\n",
    "    sns.barplot(x='Importância', y='Feature', data=top_features, palette='viridis', hue='Feature', legend=False)\n",
    "    plt.title(f'Top 15 Variáveis Mais Importantes (Manual) - {nome_modelo}', fontsize=15)\n",
    "    plt.xlabel('Importância (Queda no ROC AUC)', fontsize=12)\n",
    "    plt.ylabel('Variável', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/{nome_modelo}_importancia_variaveis_manual.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Visualizar importância com barras de erro\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    top_features = importancia.head(10)\n",
    "    plt.errorbar(\n",
    "        x=top_features['Importância'],\n",
    "        y=range(len(top_features)),\n",
    "        xerr=top_features['Desvio Padrão'],  # Nota: std aqui é do score permutado, não da importância\n",
    "        fmt='o',\n",
    "        capsize=5,\n",
    "        color='blue'\n",
    "    )\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.title(f'Top 10 Variáveis com Desvio Padrão (Manual) - {nome_modelo}', fontsize=15)\n",
    "    plt.xlabel('Importância (Queda no ROC AUC)', fontsize=12)\n",
    "    plt.ylabel('Variável', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/{nome_modelo}_importancia_variaveis_erro_manual.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    return importancia\n",
    "\n",
    "\n",
    "def analisar_shap_values(modelo, x_test, feature_names, nome_modelo, max_display=10):\n",
    "    \"\"\"\n",
    "    Analisa a importância das variáveis usando SHAP values.\n",
    "\n",
    "    Args:\n",
    "        modelo (tf.keras.Model): Modelo treinado\n",
    "        x_test (np.ndarray): Dados de teste\n",
    "        feature_names (list): Nomes das features\n",
    "        nome_modelo (str): Nome do modelo para salvar resultados\n",
    "        max_display (int): Número máximo de features para exibir\n",
    "\n",
    "    Returns:\n",
    "        tuple: (explainer, shap_values)\n",
    "    \"\"\"\n",
    "    logger.info(f\"Analisando SHAP values para o modelo {nome_modelo}\")\n",
    "\n",
    "    # Verificar se é necessário reshape para CNN ou modelos híbridos\n",
    "    x_test_2d = x_test\n",
    "    if 'cnn' in nome_modelo.lower() or 'hibrido' in nome_modelo.lower():\n",
    "        if len(x_test.shape) == 3:\n",
    "            logger.info(\"Convertendo dados 3D para 2D para análise SHAP\")\n",
    "            x_test_2d = x_test.reshape(x_test.shape[0], x_test.shape[1])\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Usar uma amostra dos dados de teste para eficiência\n",
    "        sample_size = min(100, x_test_2d.shape[0])\n",
    "        x_sample = x_test_2d[:sample_size]\n",
    "\n",
    "        # Garantir que x_sample está 2D para o SHAP\n",
    "        if len(x_sample.shape) == 3 and x_sample.shape[2] == 1:\n",
    "            logger.info(\"Convertendo dados 3D para 2D para análise SHAP\")\n",
    "            x_sample_2d = x_sample.reshape(x_sample.shape[0], x_sample.shape[1])\n",
    "        else:\n",
    "            x_sample_2d = x_sample\n",
    "\n",
    "        # Calcular SHAP values\n",
    "        logger.info(\"Calculando SHAP values (pode levar algum tempo)\")\n",
    "        explainer = shap.Explainer(modelo, x_sample_2d)\n",
    "        shap_values = explainer.shap_values(x_sample_2d)\n",
    "\n",
    "        # Resumo das contribuições das features\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        shap.summary_plot(\n",
    "            shap_values,\n",
    "            x_sample_2d,\n",
    "            feature_names=feature_names[:x_test_2d.shape[1]],\n",
    "            max_display=max_display,\n",
    "            show=False\n",
    "        )\n",
    "        plt.title(f'SHAP Summary Plot - {nome_modelo}', fontsize=15)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{RESULTS_DIR}/graficos/shap/{nome_modelo}_shap_summary.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # Gráfico de barras com importância média absoluta\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        shap.summary_plot(\n",
    "            shap_values,\n",
    "            x_sample_2d,\n",
    "            feature_names=feature_names[:x_test_2d.shape[1]],\n",
    "            plot_type='bar',\n",
    "            max_display=max_display,\n",
    "            show=False\n",
    "        )\n",
    "        plt.title(f'SHAP Feature Importance - {nome_modelo}', fontsize=15)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{RESULTS_DIR}/graficos/shap/{nome_modelo}_shap_importance.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # Gráficos de dependência para as top 3 features\n",
    "        shap_df = pd.DataFrame(shap_values, columns=feature_names[:x_test_2d.shape[1]])\n",
    "        mean_abs_shap = np.abs(shap_df).mean().sort_values(ascending=False)\n",
    "        top_features = mean_abs_shap.index[:3].tolist()\n",
    "\n",
    "        for feature in top_features:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            feature_idx = list(feature_names[:x_test_2d.shape[1]]).index(feature)\n",
    "            shap.dependence_plot(\n",
    "                feature_idx,\n",
    "                shap_values,\n",
    "                x_sample_2d,\n",
    "                feature_names=feature_names[:x_test_2d.shape[1]],\n",
    "                show=False\n",
    "            )\n",
    "            plt.title(f'SHAP Dependence Plot - {feature} - {nome_modelo}', fontsize=15)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{RESULTS_DIR}/graficos/shap/{nome_modelo}_shap_dependence_{feature}.png\", dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "        # Gráfico de força para algumas amostras individuais\n",
    "        for i in range(min(3, x_sample_2d.shape[0])):\n",
    "            plt.figure(figsize=(16, 6))\n",
    "            shap.force_plot(\n",
    "                explainer.expected_value,\n",
    "                shap_values[i],\n",
    "                x_sample_2d[i],\n",
    "                feature_names=feature_names[:x_test_2d.shape[1]],\n",
    "                matplotlib=True,\n",
    "                show=False\n",
    "            )\n",
    "            plt.title(f'SHAP Force Plot - Amostra {i + 1} - {nome_modelo}', fontsize=15)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{RESULTS_DIR}/graficos/shap/{nome_modelo}_shap_force_plot_sample_{i + 1}.png\", dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "        return explainer, shap_values\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao calcular SHAP values: {e}\")\n",
    "        logger.info(\"Continuando com outras análises...\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def analisar_lime(modelo, x_train, x_test, y_test, feature_names, nome_modelo, num_amostras=3):\n",
    "    \"\"\"\n",
    "    Analisa o modelo usando LIME (Local Interpretable Model-agnostic Explanations).\n",
    "\n",
    "    Args:\n",
    "        modelo (tf.keras.Model): Modelo treinado\n",
    "        x_train (np.ndarray): Dados de treinamento\n",
    "        x_test (np.ndarray): Dados de teste\n",
    "        y_test (np.ndarray): Rótulos de teste\n",
    "        feature_names (list): Nomes das features\n",
    "        nome_modelo (str): Nome do modelo para salvar resultados\n",
    "        num_amostras (int): Número de amostras para explicar\n",
    "\n",
    "    Returns:\n",
    "        lime.lime_tabular.LimeTabularExplainer: Explainer LIME\n",
    "    \"\"\"\n",
    "    logger.info(f\"Analisando modelo {nome_modelo} usando LIME\")\n",
    "\n",
    "    # Verificar se é necessário reshape para CNN ou modelos híbridos\n",
    "    x_train_2d = x_train\n",
    "    x_test_2d = x_test\n",
    "    if 'cnn' in nome_modelo.lower() or 'hibrido' in nome_modelo.lower():\n",
    "        if len(x_train.shape) == 3:\n",
    "            logger.info(\"Convertendo dados 3D para 2D para análise LIME\")\n",
    "            x_train_2d = x_train.reshape(x_train.shape[0], x_train.shape[1])\n",
    "            x_test_2d = x_test.reshape(x_test.shape[0], x_test.shape[1])\n",
    "\n",
    "    # Função de predição para LIME\n",
    "    def predict_func(x):\n",
    "        # Garantir que X seja um array numpy e tenha o formato correto\n",
    "        if not isinstance(x, np.ndarray):\n",
    "            x = np.array(x)\n",
    "\n",
    "        # Garantir que X seja 2D\n",
    "        if len(x.shape) == 1:\n",
    "            x = x.reshape(1, -1)\n",
    "\n",
    "        # Garantir que X tenha o número correto de features\n",
    "        if x.shape[1] != x_test_2d.shape[1]:\n",
    "            logger.warning(f\"Shape mismatch: esperado {x_test_2d.shape[1]} features, recebido {x.shape[1]}\")\n",
    "            return np.array([[0.5, 0.5]] * x.shape[0])\n",
    "\n",
    "        try:\n",
    "            if 'cnn' in nome_modelo.lower() or 'hibrido' in nome_modelo.lower():\n",
    "                x_reshaped = x.reshape(x.shape[0], x.shape[1], 1)\n",
    "                proba = modelo.predict(x_reshaped, verbose=0)\n",
    "            else:\n",
    "                proba = modelo.predict(x, verbose=0)\n",
    "\n",
    "            # Garante que proba é 1D para a classe positiva\n",
    "            proba_pos = proba.reshape(-1)\n",
    "\n",
    "            # Clamp probabilidades para evitar valores extremos\n",
    "            proba_pos = np.clip(proba_pos, 1e-7, 1 - 1e-7)\n",
    "\n",
    "            # Retorna as probabilidades para as duas classes\n",
    "            proba_neg = 1 - proba_pos\n",
    "            return np.column_stack([proba_neg, proba_pos])\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Erro na predição LIME: {e}. Retornando probabilidades padrão.\")\n",
    "            # Retornar probabilidades padrão em caso de erro\n",
    "            return np.array([[0.5, 0.5]] * x.shape[0])\n",
    "\n",
    "    try:\n",
    "        # Criar explainer LIME\n",
    "        logger.info(\"Criando LIME explainer\")\n",
    "        explainer = lime_tabular.LimeTabularExplainer(\n",
    "            x_train_2d,\n",
    "            feature_names=feature_names[:x_test_2d.shape[1]],\n",
    "            class_names=['Não Diabetes', 'Diabetes'],\n",
    "            mode='classification'\n",
    "        )\n",
    "\n",
    "        # Selecionar amostras para explicar\n",
    "        amostras_para_explicar = np.random.choice(x_test.shape[0], size=num_amostras, replace=False)\n",
    "\n",
    "        for i in amostras_para_explicar:\n",
    "            logger.info(f\"Explicando amostra {i}\")\n",
    "\n",
    "            # Gerar explicação\n",
    "            exp = explainer.explain_instance(\n",
    "                x_test_2d[i],\n",
    "                predict_func,\n",
    "                num_features=10\n",
    "            )\n",
    "\n",
    "            # Visualizar explicação\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            exp.as_pyplot_figure()\n",
    "            plt.title(f'Explicação LIME - Amostra {i} - {nome_modelo}', fontsize=15)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{RESULTS_DIR}/graficos/lime/{nome_modelo}_lime_amostra_{i}.png\", dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "        return explainer\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao gerar explicações LIME: {e}\")\n",
    "        logger.info(\"Continuando com outras análises...\")\n",
    "        return None\n"
   ],
   "id": "f8748e0b4e0c49bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Carregar e analisar dados\n",
    "df = carregar_dados(\"diabetes_prediction_dataset.csv\")\n",
    "analisar_dados(df)\n",
    "\n",
    "# 2. Pré-processamento dos dados\n",
    "x_train, X_val, x_test, y_train, y_val, y_test, pre_processador, feature_names = pre_processar_dados(\n",
    "    df,\n",
    "    metodo_normalizacao='standard',\n",
    "    metodo_balanceamento='smote',\n",
    "    tratar_outliers=True\n",
    ")\n",
    "\n",
    "# Reshape para CNN\n",
    "x_train_cnn = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "X_val_cnn = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
    "X_test_cnn = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)\n"
   ],
   "id": "7528c487bd514aef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Treinar modelos clássicos\n",
    "resultados_classicos = treinar_modelos_classicos(x_train, y_train, x_test, y_test, feature_names)"
   ],
   "id": "d9397016a0ef8788",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Treinar e avaliar modelo MLP\n",
    "logger.info(\"Treinando modelo MLP\")\n",
    "modelo_mlp = criar_modelo_mlp(\n",
    "    input_shape=(x_train.shape[1],),\n",
    "    camadas=[128, 64, 32],\n",
    "    activations=['relu', 'relu', 'relu'],\n",
    "    dropout_rates=[0.3, 0.3, 0.3],\n",
    "    regularization=0.001,\n",
    "    learning_rate=0.001,\n",
    "    batch_norm=True\n",
    ")\n",
    "\n",
    "treinar_modelo(\n",
    "    modelo_mlp, x_train, y_train, X_val, y_val,\n",
    "    nome_modelo=\"mlp\",\n",
    "    batch_size=128,\n",
    "    epochs=100\n",
    ")"
   ],
   "id": "15f9bfa5a21ca7e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 5. Treinar e avaliar modelo CNN\n",
    "logger.info(\"Treinando modelo CNN\")\n",
    "modelo_cnn = criar_modelo_cnn(\n",
    "    input_shape=(x_train.shape[1], 1),\n",
    "    filtros=[64, 32, 16],\n",
    "    kernel_sizes=[3, 3, 3],\n",
    "    ativacoes=['relu', 'relu', 'relu'],\n",
    "    dropout_rates=[0.3, 0.3, 0.3],\n",
    "    pool_sizes=[2, 2, 2],\n",
    "    regularizacao=0.001,\n",
    "    learning_rate=0.005,\n",
    "    batch_norm=True\n",
    ")\n",
    "\n",
    "treinar_modelo(\n",
    "    modelo_cnn, x_train_cnn, y_train, X_val_cnn, y_val,\n",
    "    nome_modelo=\"cnn\",\n",
    "    batch_size=128,\n",
    "    epochs=100,\n",
    ")"
   ],
   "id": "52add393f9d06682",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 6. Treinar e avaliar modelo híbrido\n",
    "logger.info(\"Treinando modelo híbrido CNN-LSTM\")\n",
    "modelo_hibrido = criar_modelo_hibrido(\n",
    "    input_shape=(x_train.shape[1], 1),\n",
    "    filtros_cnn=[64, 32, 16],\n",
    "    kernel_sizes=[3, 3, 3],\n",
    "    unidades_lstm=32,\n",
    "    unidades_densas=[64, 32, 16],\n",
    "    ativacoes=['relu', 'relu', 'relu'],\n",
    "    dropout_rates=[0.3, 0.3, 0.3],\n",
    "    regularizacao=0.001,\n",
    "    learning_rate=0.001,\n",
    "    batch_norm=True\n",
    ")\n",
    "\n",
    "treinar_modelo(\n",
    "    modelo_hibrido, x_train_cnn, y_train, X_val_cnn, y_val,\n",
    "    nome_modelo=\"hibrido\",\n",
    "    batch_size=128,\n",
    "    epochs=100\n",
    ")"
   ],
   "id": "cd6dd53c7f234d0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Caminho do arquivo onde o modelo foi salvo\n",
    "caminho_modelo_mlp = f\"{RESULTS_DIR}/modelos/mlp_final.h5\"\n",
    "caminho_history_mlp = f\"{RESULTS_DIR}/history/mlp_history.csv\"\n",
    "caminho_modelo_cnn = f\"{RESULTS_DIR}/modelos/cnn_final.h5\"\n",
    "caminho_history_cnn = f\"{RESULTS_DIR}/history/cnn_history.csv\"\n",
    "caminho_modelo_hibrido = f\"{RESULTS_DIR}/modelos/hibrido_final.h5\"\n",
    "caminho_history_hibrido = f\"{RESULTS_DIR}/history/hibrido_history.csv\"\n",
    "\n",
    "# Carregar o modelo híbrido\n",
    "modelo_hibrido_treinado = load_model(caminho_modelo_hibrido)\n",
    "historico_hibrido = pd.read_csv(caminho_history_hibrido)\n",
    "\n",
    "# Carregar o modelo MLP\n",
    "modelo_mlp_treinado = load_model(caminho_modelo_mlp)\n",
    "historico_mlp = pd.read_csv(caminho_history_cnn)\n",
    "\n",
    "# Carregar o modelo CNN\n",
    "modelo_cnn_treinado = load_model(caminho_modelo_cnn)\n",
    "historico_cnn = pd.read_csv(caminho_history_mlp)"
   ],
   "id": "11d43641ef5ca4a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Avaliar modelos\n",
    "metricas_mlp, y_pred_mlp, y_prob_mlp = avaliar_modelo(\n",
    "    modelo_mlp_treinado, x_test, y_test, nome_modelo=\"mlp\"\n",
    ")\n",
    "\n",
    "visualizar_historico_treinamento(historico_mlp, \"mlp\")\n",
    "\n",
    "metricas_hibrido, y_pred_hibrido, y_prob_hibrido = avaliar_modelo(\n",
    "    modelo_hibrido_treinado, X_test_cnn, y_test, nome_modelo=\"hibrido\"\n",
    ")\n",
    "\n",
    "visualizar_historico_treinamento(historico_hibrido, \"hibrido\")\n",
    "\n",
    "metricas_cnn, y_pred_cnn, y_prob_cnn = avaliar_modelo(\n",
    "    modelo_cnn_treinado, X_test_cnn, y_test, nome_modelo=\"cnn\"\n",
    ")\n",
    "\n",
    "visualizar_historico_treinamento(historico_cnn, \"cnn\")"
   ],
   "id": "11263b6d85abd65a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 7. Análise de importância de variáveis\n",
    "\n",
    "importancia_mlp = analisar_importancia_variaveis(\n",
    "    modelo_mlp_treinado, x_test, y_test, feature_names, \"mlp\"\n",
    ")\n",
    "\n",
    "importancia_cnn = analisar_importancia_variaveis(\n",
    "    modelo_cnn_treinado, X_test_cnn, y_test, feature_names, \"cnn\"\n",
    ")\n",
    "\n",
    "importancia_hibrido = analisar_importancia_variaveis(\n",
    "    modelo_hibrido_treinado, X_test_cnn, y_test, feature_names, \"hibrido\"\n",
    ")"
   ],
   "id": "490ce0913a1cc511",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 8. Análise SHAP\n",
    "explainer_mlp, shap_values_mlp = analisar_shap_values(\n",
    "    modelo_mlp_treinado, x_test, feature_names, \"mlp\"\n",
    ")\n",
    "\n",
    "explainer_cnn, shap_values_cnn = analisar_shap_values(\n",
    "    modelo_cnn_treinado, X_test_cnn, feature_names, \"cnn\"\n",
    ")\n",
    "\n",
    "explainer_cnn, shap_values_cnn = analisar_shap_values(\n",
    "    modelo_cnn_treinado, X_test_cnn, feature_names, \"hibrido\"\n",
    ")"
   ],
   "id": "cd1fea82a171af69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 9. Análise LIME\n",
    "lime_explainer_mlp = analisar_lime(\n",
    "    modelo_mlp_treinado, x_train, x_test, y_test, feature_names, \"mlp\"\n",
    ")\n",
    "\n",
    "lime_explainer_cnn = analisar_lime(\n",
    "    modelo_cnn_treinado, x_train_cnn, X_test_cnn, y_test, feature_names, \"cnn\"\n",
    ")\n",
    "\n",
    "lime_explainer_hibrido = analisar_lime(\n",
    "    modelo_hibrido_treinado, x_train_cnn, X_test_cnn, y_test, feature_names, \"hibrido\"\n",
    ")\n"
   ],
   "id": "2ab6c489c0660fa9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Corrigir carregamento dos resultados dos modelos clássicos\n",
    "try:\n",
    "    # Tentar carregar o arquivo correto de resultados clássicos\n",
    "    if os.path.exists(f\"{RESULTS_DIR}/resultados_modelos_classicos.csv\"):\n",
    "        resultados_classicos = pd.read_csv(f\"{RESULTS_DIR}/resultados_modelos_classicos.csv\")\n",
    "        logger.info(f\"Resultados clássicos carregados: {len(resultados_classicos)} modelos\")\n",
    "    else:\n",
    "        logger.error(\"Arquivo de resultados clássicos não encontrado\")\n",
    "        # Criar DataFrame vazio com estrutura correta\n",
    "        resultados_classicos = pd.DataFrame(columns=['modelo', 'accuracy', 'balanced_accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'average_precision'])\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erro ao carregar resultados clássicos: {e}\")\n",
    "    resultados_classicos = pd.DataFrame(columns=['modelo', 'accuracy', 'balanced_accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'average_precision'])\n",
    "\n",
    "# 10. Comparação de todos os modelos - com validação de NaN\n",
    "try:\n",
    "    # Verificar se as métricas dos modelos estão disponíveis e válidas\n",
    "    modelos_validos = []\n",
    "\n",
    "    # Validar MLP\n",
    "    if 'metricas_mlp' in locals() and all(key in metricas_mlp for key in ['accuracy', 'balanced_accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'average_precision']):\n",
    "        if not any(pd.isna(list(metricas_mlp.values()))):\n",
    "            modelos_validos.append({\n",
    "                'modelo': 'MLP',\n",
    "                'accuracy': metricas_mlp['accuracy'],\n",
    "                'balanced_accuracy': metricas_mlp['balanced_accuracy'],\n",
    "                'precision': metricas_mlp['precision'],\n",
    "                'recall': metricas_mlp['recall'],\n",
    "                'f1': metricas_mlp['f1'],\n",
    "                'roc_auc': metricas_mlp['roc_auc'],\n",
    "                'average_precision': metricas_mlp['average_precision']\n",
    "            })\n",
    "            logger.info(\"MLP: métricas válidas adicionadas\")\n",
    "        else:\n",
    "            logger.warning(\"MLP: métricas contêm valores NaN\")\n",
    "    else:\n",
    "        logger.warning(\"MLP: métricas não disponíveis ou incompletas\")\n",
    "\n",
    "    # Validar CNN\n",
    "    if 'metricas_cnn' in locals() and all(key in metricas_cnn for key in ['accuracy', 'balanced_accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'average_precision']):\n",
    "        if not any(pd.isna(list(metricas_cnn.values()))):\n",
    "            modelos_validos.append({\n",
    "                'modelo': 'CNN',\n",
    "                'accuracy': metricas_cnn['accuracy'],\n",
    "                'balanced_accuracy': metricas_cnn['balanced_accuracy'],\n",
    "                'precision': metricas_cnn['precision'],\n",
    "                'recall': metricas_cnn['recall'],\n",
    "                'f1': metricas_cnn['f1'],\n",
    "                'roc_auc': metricas_cnn['roc_auc'],\n",
    "                'average_precision': metricas_cnn['average_precision']\n",
    "            })\n",
    "            logger.info(\"CNN: métricas válidas adicionadas\")\n",
    "        else:\n",
    "            logger.warning(\"CNN: métricas contêm valores NaN\")\n",
    "    else:\n",
    "        logger.warning(\"CNN: métricas não disponíveis ou incompletas\")\n",
    "\n",
    "    # Validar Híbrido\n",
    "    if 'metricas_hibrido' in locals() and all(key in metricas_hibrido for key in ['accuracy', 'balanced_accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'average_precision']):\n",
    "        if not any(pd.isna(list(metricas_hibrido.values()))):\n",
    "            modelos_validos.append({\n",
    "                'modelo': 'Híbrido CNN-LSTM',\n",
    "                'accuracy': metricas_hibrido['accuracy'],\n",
    "                'balanced_accuracy': metricas_hibrido['balanced_accuracy'],\n",
    "                'precision': metricas_hibrido['precision'],\n",
    "                'recall': metricas_hibrido['recall'],\n",
    "                'f1': metricas_hibrido['f1'],\n",
    "                'roc_auc': metricas_hibrido['roc_auc'],\n",
    "                'average_precision': metricas_hibrido['average_precision']\n",
    "            })\n",
    "            logger.info(\"Híbrido: métricas válidas adicionadas\")\n",
    "        else:\n",
    "            logger.warning(\"Híbrido: métricas contêm valores NaN\")\n",
    "    else:\n",
    "        logger.warning(\"Híbrido: métricas não disponíveis ou incompletas\")\n",
    "\n",
    "    # Criar DataFrame com modelos válidos\n",
    "    if modelos_validos:\n",
    "        resultados_redes = pd.DataFrame(modelos_validos)\n",
    "        logger.info(f\"Criado DataFrame com {len(resultados_redes)} modelos de redes neurais válidos\")\n",
    "    else:\n",
    "        logger.warning(\"Nenhum modelo de rede neural válido encontrado\")\n",
    "        resultados_redes = pd.DataFrame(columns=['modelo', 'accuracy', 'balanced_accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'average_precision'])\n",
    "\n",
    "    # Executar comparação apenas se houver dados válidos\n",
    "    if len(resultados_classicos) > 0 or len(resultados_redes) > 0:\n",
    "        comparar_todos_modelos(resultados_classicos, resultados_redes)\n",
    "        logger.info(\"Comparação de modelos executada com sucesso\")\n",
    "    else:\n",
    "        logger.error(\"Nenhum resultado válido disponível para comparação\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erro na comparação de modelos: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ],
   "id": "c4794cdd955c285f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
