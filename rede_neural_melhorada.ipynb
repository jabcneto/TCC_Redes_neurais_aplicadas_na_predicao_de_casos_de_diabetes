{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T14:40:45.810838Z",
     "start_time": "2025-07-01T14:40:45.630374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =====================================================================\n",
    "# IMPORTAÇÃO DE BIBLIOTECAS\n",
    "# =====================================================================\n",
    "\n",
    "# Manipulação e análise de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Visualização\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Pré-processamento e splitting\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Métricas de avaliação\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, confusion_matrix,\n",
    "    f1_score, precision_score, recall_score, roc_auc_score, roc_curve, auc,\n",
    "    precision_recall_curve, average_precision_score, classification_report\n",
    ")\n",
    "\n",
    "# Modelos clássicos\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Modelos avançados (boosting)\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Redes neurais (Keras/TensorFlow)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model, load_model, save_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Activation, PReLU, LeakyReLU, Dense, Dropout,\n",
    "    Conv1D, MaxPooling1D, Flatten, BatchNormalization,\n",
    "    LSTM, Bidirectional, GRU, GlobalAveragePooling1D,\n",
    "    Add, Concatenate\n",
    ")\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau,\n",
    "    TensorBoard, CSVLogger\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Balanceamento de dados\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Interpretabilidade\n",
    "import shap\n",
    "from sklearn.inspection import permutation_importance\n",
    "from lime import lime_tabular"
   ],
   "id": "2440bd5a92574909",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jabcn\\OneDrive\\Documents\\Faculdade\\UCP\\python\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T14:40:48.438220Z",
     "start_time": "2025-07-01T14:40:48.413595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =====================================================================\n",
    "# CONFIGURAÇÕES GERAIS\n",
    "# =====================================================================\n",
    "logging.basicConfig(\n",
    "    level=logging.ERROR,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"diabetes_prediction.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Reduzir verbosidade do TensorFlow\n",
    "\n",
    "# Criar diretório para salvar resultados\n",
    "RESULTS_DIR = \"resultados_diabetes\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{RESULTS_DIR}/modelos\", exist_ok=True)\n",
    "os.makedirs(f\"{RESULTS_DIR}/graficos/confusao\", exist_ok=True)\n",
    "os.makedirs(f\"{RESULTS_DIR}/graficos/roc\", exist_ok=True)\n",
    "os.makedirs(f\"{RESULTS_DIR}/graficos/importancia\", exist_ok=True)\n",
    "os.makedirs(f\"{RESULTS_DIR}/graficos/distribuicao\", exist_ok=True)\n",
    "os.makedirs(f\"{RESULTS_DIR}/logs\", exist_ok=True)"
   ],
   "id": "d72f0d25ad005192",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T14:40:51.316527Z",
     "start_time": "2025-07-01T14:40:51.288532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =====================================================================\n",
    "# FUNÇÕES DE CARREGAMENTO E PRÉ-PROCESSAMENTO DE DADOS\n",
    "# =====================================================================\n",
    "\n",
    "def carregar_dados(caminho_arquivo, verbose=True):\n",
    "    \"\"\"\n",
    "    Carrega o dataset de predição de diabetes e realiza análise exploratória inicial.\n",
    "\n",
    "    Args:\n",
    "        caminho_arquivo (str): Caminho para o arquivo CSV do dataset\n",
    "        verbose (bool): Se True, exibe informações sobre o dataset\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame com os dados carregados\n",
    "    \"\"\"\n",
    "    logger.info(f\"Carregando dados de {caminho_arquivo}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(caminho_arquivo)\n",
    "        if verbose:\n",
    "            logger.info(f\"Dataset carregado com sucesso. Formato: {df.shape}\")\n",
    "            logger.info(f\"Colunas: {df.columns.tolist()}\")\n",
    "            logger.info(f\"Tipos de dados:\\n{df.dtypes}\")\n",
    "            logger.info(\n",
    "                f\"Distribuição da variável alvo (diabetes):\\n{df['diabetes'].value_counts(normalize=True) * 100}\")\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao carregar o dataset: {e}\")\n",
    "        logger.info(\"Criando dataset sintético para demonstração...\")\n",
    "\n",
    "        # Criar dataset sintético para demonstração\n",
    "        np.random.seed(42)\n",
    "        n_samples = 1000\n",
    "\n",
    "        # Criar variáveis sintéticas\n",
    "        gender = np.random.choice(['Female', 'Male'], size=n_samples, p=[0.59, 0.41])\n",
    "        age = np.random.normal(loc=45, scale=15, size=n_samples).astype(int)\n",
    "        age = np.clip(age, 18, 90)\n",
    "\n",
    "        hypertension = np.random.choice([0, 1], size=n_samples, p=[0.75, 0.25])\n",
    "        heart_disease = np.random.choice([0, 1], size=n_samples, p=[0.85, 0.15])\n",
    "\n",
    "        smoking_history = np.random.choice(\n",
    "            ['never', 'former', 'current', 'not current', 'ever'],\n",
    "            size=n_samples,\n",
    "            p=[0.4, 0.2, 0.2, 0.1, 0.1]\n",
    "        )\n",
    "\n",
    "        bmi = np.random.normal(loc=28, scale=6, size=n_samples)\n",
    "        bmi = np.clip(bmi, 15, 50)\n",
    "\n",
    "        # Gerar HbA1c e glicose com correlação\n",
    "        base_values = np.random.normal(loc=0, scale=1, size=n_samples)\n",
    "        hba1c = 5.5 + base_values * 1.5\n",
    "        hba1c = np.clip(hba1c, 3.5, 9.0)\n",
    "\n",
    "        glucose = 100 + base_values * 40\n",
    "        glucose = np.clip(glucose, 70, 300)\n",
    "\n",
    "        # Gerar diabetes com base em HbA1c e glicose\n",
    "        prob_diabetes = 1 / (1 + np.exp(-(hba1c - 6.5) * 2 - (\n",
    "                glucose - 140) / 30 + age / 100 + bmi / 50 + hypertension * 0.5 + heart_disease * 0.5))\n",
    "        diabetes = np.random.binomial(1, prob_diabetes)\n",
    "\n",
    "        # Criar DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'gender': gender,\n",
    "            'age': age,\n",
    "            'hypertension': hypertension,\n",
    "            'heart_disease': heart_disease,\n",
    "            'smoking_history': smoking_history,\n",
    "            'bmi': bmi,\n",
    "            'HbA1c_level': hba1c,\n",
    "            'blood_glucose_level': glucose,\n",
    "            'diabetes': diabetes\n",
    "        })\n",
    "\n",
    "        if verbose:\n",
    "            logger.info(f\"Dataset sintético criado. Formato: {df.shape}\")\n",
    "            logger.info(\n",
    "                f\"Distribuição da variável alvo (diabetes):\\n{df['diabetes'].value_counts(normalize=True) * 100}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "def analisar_dados(df):\n",
    "    \"\"\"\n",
    "    Realiza análise exploratória detalhada dos dados.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame com os dados\n",
    "\n",
    "    Returns:\n",
    "        dict: Dicionário com estatísticas e informações da análise\n",
    "    \"\"\"\n",
    "    logger.info(\"Realizando análise exploratória dos dados\")\n",
    "\n",
    "    # Estatísticas básicas\n",
    "    estatisticas = {\n",
    "        'shape': df.shape,\n",
    "        'missing_values': df.isnull().sum().to_dict(),\n",
    "        'dtypes': df.dtypes.to_dict(),\n",
    "        'target_distribution': df['diabetes'].value_counts(normalize=True).to_dict(),\n",
    "        'numeric_stats': df.describe().to_dict(),\n",
    "    }\n",
    "\n",
    "    # Análise de variáveis categóricas\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    estatisticas['categorical_counts'] = {col: df[col].value_counts().to_dict() for col in cat_cols}\n",
    "\n",
    "    # Detecção de outliers (usando IQR)\n",
    "    num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    outliers = {}\n",
    "    for col in num_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers[col] = {\n",
    "            'lower_bound': lower_bound,\n",
    "            'upper_bound': upper_bound,\n",
    "            'n_outliers': ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
    "        }\n",
    "    estatisticas['outliers'] = outliers\n",
    "\n",
    "    # Correlações\n",
    "    # Garante que apenas colunas numéricas sejam usadas para correlação\n",
    "    numeric_df = df.select_dtypes(include=np.number)\n",
    "    if 'diabetes' in numeric_df.columns:\n",
    "        estatisticas['correlations'] = numeric_df.corr()['diabetes'].to_dict()\n",
    "    else:\n",
    "        estatisticas['correlations'] = {}\n",
    "        logger.warning(\"Coluna 'diabetes' não encontrada ou não é numérica para cálculo de correlação.\")\n",
    "\n",
    "\n",
    "    # Visualizações\n",
    "    visualizar_analise_exploratoria(df)\n",
    "\n",
    "    return estatisticas\n",
    "\n",
    "\n",
    "def visualizar_analise_exploratoria(dataframe):\n",
    "    \"\"\"\n",
    "    Cria visualizações para análise exploratória dos dados.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): DataFrame com os dados\n",
    "    \"\"\"\n",
    "    logger.info(\"Gerando visualizações para análise exploratória\")\n",
    "    df = dataframe.select_dtypes(include=np.number)\n",
    "    # Configuração para visualizações\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    sns.set_palette('viridis')\n",
    "\n",
    "    # 1. Distribuição da variável alvo\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.countplot(x='diabetes', data=df, palette=['#3498db', '#e74c3c'])\n",
    "    plt.title('Distribuição da Variável Alvo (Diabetes)', fontsize=15)\n",
    "    plt.xlabel('Diabetes', fontsize=12)\n",
    "    plt.ylabel('Contagem', fontsize=12)\n",
    "\n",
    "    # Adicionar percentagens\n",
    "    total = len(df)\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x() + p.get_width() / 2., height + 5,\n",
    "                f'{height} ({height / total:.1%})',\n",
    "                ha=\"center\", fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/distribuicao/distribuicao_target.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Distribuição das variáveis numéricas por status de diabetes\n",
    "    num_cols = ['age', 'bmi', 'HbA1c_level', 'blood_glucose_level']\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    for i, col in enumerate(num_cols):\n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        sns.histplot(data=df, x=col, hue='diabetes', kde=True, bins=30,\n",
    "                     palette=['#3498db', '#e74c3c'], alpha=0.6)\n",
    "        plt.title(f'Distribuição de {col} por Status de Diabetes', fontsize=13)\n",
    "        plt.xlabel(col, fontsize=11)\n",
    "        plt.ylabel('Contagem', fontsize=11)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/distribuicao/distribuicao_variaveis_numericas.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 3. Boxplots para variáveis numéricas\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    for i, col in enumerate(num_cols):\n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        sns.boxplot(x='diabetes', y=col, data=df, palette=['#3498db', '#e74c3c'])\n",
    "        plt.title(f'{col} por Status de Diabetes', fontsize=13)\n",
    "        plt.xlabel('Diabetes', fontsize=11)\n",
    "        plt.ylabel(col, fontsize=11)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/boxplots_variaveis_numericas.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 4. Matriz de correlação\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    corr_matrix = df.corr()\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "                square=True, linewidths=0.5)\n",
    "    plt.title('Matriz de Correlação', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/matriz_correlacao.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 5. Pairplot para variáveis numéricas\n",
    "    sns.pairplot(df[num_cols + ['diabetes']], hue='diabetes',\n",
    "                 palette=['#3498db', '#e74c3c'], diag_kind='kde')\n",
    "    plt.suptitle('Pairplot de Variáveis Numéricas', y=1.02, fontsize=16)\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/pairplot_variaveis_numericas.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 6. Contagem de variáveis categóricas\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    if cat_cols:\n",
    "        plt.figure(figsize=(15, 5 * len(cat_cols)))\n",
    "\n",
    "        for i, col in enumerate(cat_cols):\n",
    "            plt.subplot(len(cat_cols), 1, i + 1)\n",
    "            sns.countplot(x=col, hue='diabetes', data=df, palette=['#3498db', '#e74c3c'])\n",
    "            plt.title(f'Distribuição de {col} por Status de Diabetes', fontsize=13)\n",
    "            plt.xlabel(col, fontsize=11)\n",
    "            plt.ylabel('Contagem', fontsize=11)\n",
    "            plt.xticks(rotation=45)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{RESULTS_DIR}/graficos/distribuicao/distribuicao_variaveis_categoricas.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    # 7. Relação entre HbA1c e glicose com diabetes\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    scatter = sns.scatterplot(data=df, x='HbA1c_level', y='blood_glucose_level',\n",
    "                              hue='diabetes', palette=['#3498db', '#e74c3c'],\n",
    "                              s=80, alpha=0.7)\n",
    "    plt.axvline(x=6.5, color='red', linestyle='--', label='Limiar HbA1c (6.5%)')\n",
    "    plt.axhline(y=126, color='green', linestyle='--', label='Limiar Glicose (126 mg/dL)')\n",
    "\n",
    "    # Adicionar anotações para os quadrantes\n",
    "    plt.text(7.5, 200, 'Alto risco\\n(HbA1c alto, Glicose alta)', fontsize=12, ha='center')\n",
    "    plt.text(5.5, 200, 'Risco moderado\\n(HbA1c normal, Glicose alta)', fontsize=12, ha='center')\n",
    "    plt.text(7.5, 100, 'Risco moderado\\n(HbA1c alto, Glicose normal)', fontsize=12, ha='center')\n",
    "    plt.text(5.5, 100, 'Baixo risco\\n(HbA1c normal, Glicose normal)', fontsize=12, ha='center')\n",
    "\n",
    "    plt.title('Relação entre HbA1c e Glicose no Sangue', fontsize=15)\n",
    "    plt.xlabel('Nível de HbA1c (%)', fontsize=12)\n",
    "    plt.ylabel('Nível de Glicose (mg/dL)', fontsize=12)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/relacao_hba1c_glicose.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def preprocessar_dados(df, metodo_normalizacao='standard',\n",
    "                       metodo_balanceamento='smote',\n",
    "                       selecao_features=True,\n",
    "                       tratar_outliers=True,\n",
    "                       test_size=0.2,\n",
    "                       val_size=0.15,\n",
    "                       random_state=42):\n",
    "    \"\"\"\n",
    "    Realiza o pré-processamento completo dos dados para treinamento do modelo.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame com os dados\n",
    "        metodo_normalizacao (str): Método de normalização ('standard', 'minmax', 'robust')\n",
    "        metodo_balanceamento (str): Método de balanceamento ('smote', 'adasyn', 'borderline', 'tomek', 'none')\n",
    "        selecao_features (bool): Se True, realiza seleção de features\n",
    "        tratar_outliers (bool): Se True, trata outliers\n",
    "        test_size (float): Proporção do conjunto de teste\n",
    "        val_size (float): Proporção do conjunto de validação (em relação ao conjunto não-teste)\n",
    "        random_state (int): Semente aleatória\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X_train, X_val, X_test, y_train, y_val, y_test, preprocessador, feature_names)\n",
    "    \"\"\"\n",
    "    logger.info(\"Iniciando pré-processamento dos dados\")\n",
    "\n",
    "    # Remover valores 'Other' da coluna gender (se existir)\n",
    "    if 'gender' in df.columns and 'Other' in df['gender'].unique():\n",
    "        logger.info(\"Removendo valores 'Other' da coluna gender\")\n",
    "        df = df[df['gender'] != 'Other']\n",
    "\n",
    "    # Separação entre atributos e rótulo\n",
    "    X = df.drop(columns=['diabetes'])\n",
    "    y = df['diabetes']\n",
    "\n",
    "    # Identificar colunas numéricas e categóricas\n",
    "    colunas_numericas = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    colunas_categoricas = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    logger.info(f\"Colunas numéricas: {colunas_numericas}\")\n",
    "    logger.info(f\"Colunas categóricas: {colunas_categoricas}\")\n",
    "\n",
    "    # Tratamento de outliers\n",
    "    if tratar_outliers:\n",
    "        logger.info(\"Tratando outliers usando capping\")\n",
    "        for col in colunas_numericas:\n",
    "            Q1 = X[col].quantile(0.25)\n",
    "            Q3 = X[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "            # Aplicar capping\n",
    "            X[col] = np.where(X[col] < lower_bound, lower_bound, X[col])\n",
    "            X[col] = np.where(X[col] > upper_bound, upper_bound, X[col])\n",
    "\n",
    "    # Divisão em treino, validação e teste\n",
    "    # Primeiro separamos o conjunto de teste\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    # Depois separamos o conjunto de validação do conjunto de treino\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size / (1 - test_size),\n",
    "        random_state=random_state, stratify=y_temp\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Formato dos dados de treino: {X_train.shape}\")\n",
    "    logger.info(f\"Formato dos dados de validação: {X_val.shape}\")\n",
    "    logger.info(f\"Formato dos dados de teste: {X_test.shape}\")\n",
    "\n",
    "    # Criar preprocessador\n",
    "    preprocessadores = []\n",
    "\n",
    "    # Preprocessador para variáveis numéricas\n",
    "    if metodo_normalizacao == 'standard':\n",
    "        logger.info(\"Usando StandardScaler para normalização\")\n",
    "        preprocessador_numerico = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "    elif metodo_normalizacao == 'minmax':\n",
    "        logger.info(\"Usando MinMaxScaler para normalização\")\n",
    "        preprocessador_numerico = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', MinMaxScaler())\n",
    "        ])\n",
    "    elif metodo_normalizacao == 'robust':\n",
    "        logger.info(\"Usando RobustScaler para normalização\")\n",
    "        preprocessador_numerico = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', RobustScaler())\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError(f\"Método de normalização '{metodo_normalizacao}' não suportado\")\n",
    "\n",
    "    if colunas_numericas:\n",
    "        preprocessadores.append(('num', preprocessador_numerico, colunas_numericas))\n",
    "\n",
    "    # Preprocessador para variáveis categóricas\n",
    "    if colunas_categoricas:\n",
    "        logger.info(\"Aplicando OneHotEncoding para variáveis categóricas\")\n",
    "        preprocessador_categorico = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ])\n",
    "        preprocessadores.append(('cat', preprocessador_categorico, colunas_categoricas))\n",
    "\n",
    "    # Criar preprocessador completo\n",
    "    preprocessador = ColumnTransformer(\n",
    "        transformers=preprocessadores,\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "    # Ajustar preprocessador apenas nos dados de treino\n",
    "    logger.info(\"Aplicando transformações de pré-processamento\")\n",
    "    X_train_proc = preprocessador.fit_transform(X_train)\n",
    "    X_val_proc = preprocessador.transform(X_val)\n",
    "    X_test_proc = preprocessador.transform(X_test)\n",
    "\n",
    "    # Obter nomes das features após one-hot encoding\n",
    "    feature_names = []\n",
    "\n",
    "    # Adicionar nomes das features numéricas\n",
    "    if colunas_numericas:\n",
    "        feature_names.extend(colunas_numericas)\n",
    "\n",
    "    # Adicionar nomes das features categóricas após one-hot encoding\n",
    "    if colunas_categoricas:\n",
    "        encoder = preprocessador.named_transformers_['cat'].named_steps['onehot']\n",
    "        encoded_features = encoder.get_feature_names_out(colunas_categoricas)\n",
    "        feature_names.extend(encoded_features)\n",
    "\n",
    "    # Seleção de features\n",
    "    if selecao_features and len(feature_names) > 5:\n",
    "        logger.info(\"Realizando seleção de features\")\n",
    "\n",
    "        # Usando SelectKBest com f_classif (ANOVA F-value)\n",
    "        k = min(15, len(feature_names))  # Selecionar no máximo 15 features ou todas se houver menos\n",
    "        selector = SelectKBest(f_classif, k=k)\n",
    "        X_train_proc = selector.fit_transform(X_train_proc, y_train)\n",
    "        X_val_proc = selector.transform(X_val_proc)\n",
    "        X_test_proc = selector.transform(X_test_proc)\n",
    "\n",
    "        # Atualizar nomes das features\n",
    "        selected_indices = selector.get_support(indices=True)\n",
    "        feature_names = [feature_names[i] for i in selected_indices]\n",
    "\n",
    "        logger.info(f\"Features selecionadas: {feature_names}\")\n",
    "\n",
    "    # Balanceamento de dados\n",
    "    if metodo_balanceamento != 'none':\n",
    "        logger.info(f\"Aplicando balanceamento de dados usando {metodo_balanceamento}\")\n",
    "\n",
    "        if metodo_balanceamento == 'smote':\n",
    "            balanceador = SMOTE(random_state=random_state)\n",
    "        elif metodo_balanceamento == 'adasyn':\n",
    "            balanceador = ADASYN(random_state=random_state)\n",
    "        elif metodo_balanceamento == 'borderline':\n",
    "            balanceador = BorderlineSMOTE(random_state=random_state)\n",
    "        elif metodo_balanceamento == 'tomek':\n",
    "            balanceador = SMOTETomek(random_state=random_state)\n",
    "        else:\n",
    "            raise ValueError(f\"Método de balanceamento '{metodo_balanceamento}' não suportado\")\n",
    "\n",
    "        X_train_proc, y_train = balanceador.fit_resample(X_train_proc, y_train)\n",
    "\n",
    "        logger.info(f\"Formato dos dados de treino após balanceamento: {X_train_proc.shape}\")\n",
    "        logger.info(f\"Distribuição de classes após balanceamento: {np.bincount(y_train)}\")\n",
    "\n",
    "    return X_train_proc, X_val_proc, X_test_proc, y_train, y_val, y_test, preprocessador, feature_names"
   ],
   "id": "169a4c5e7493d5c5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T14:41:03.672603Z",
     "start_time": "2025-07-01T14:41:03.656110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =====================================================================\n",
    "# FUNÇÕES PARA CRIAÇÃO DE MODELOS\n",
    "# =====================================================================\n",
    "\n",
    "def criar_modelo_mlp(input_shape,\n",
    "                     camadas=[128, 64, 32],\n",
    "                     ativacoes=['relu', 'relu', 'relu'],\n",
    "                     dropout_rates=[0.3, 0.3, 0.3],\n",
    "                     regularizacao=0.001,\n",
    "                     learning_rate=0.005,\n",
    "                     batch_norm=True):\n",
    "    \"\"\"\n",
    "    Cria um modelo MLP (Perceptron Multicamadas) para classificação binária.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Forma dos dados de entrada\n",
    "        camadas (list): Lista com o número de neurônios em cada camada oculta\n",
    "        ativacoes (list): Lista com as funções de ativação para cada camada\n",
    "        dropout_rates (list): Lista com as taxas de dropout para cada camada\n",
    "        regularizacao (float): Coeficiente de regularização L2\n",
    "        learning_rate (float): Taxa de aprendizado\n",
    "        batch_norm (bool): Se True, adiciona camadas de Batch Normalization\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: Modelo MLP compilado\n",
    "    \"\"\"\n",
    "    logger.info(f\"Criando modelo MLP com {len(camadas)} camadas ocultas\")\n",
    "    logger.info(f\"Arquitetura: {camadas}\")\n",
    "    logger.info(f\"Ativações: {ativacoes}\")\n",
    "    logger.info(f\"Dropout rates: {dropout_rates}\")\n",
    "\n",
    "    # Verificar se os parâmetros têm o mesmo comprimento\n",
    "    assert len(camadas) == len(ativacoes) == len(dropout_rates), \\\n",
    "        \"camadas, ativacoes e dropout_rates devem ter o mesmo comprimento\"\n",
    "\n",
    "    # Criar modelo\n",
    "    model = Sequential()\n",
    "\n",
    "    # Camada de entrada\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    # Camadas ocultas\n",
    "    for i, (units, activation, dropout_rate) in enumerate(zip(camadas, ativacoes, dropout_rates)):\n",
    "        # Adicionar camada densa com regularização L2\n",
    "        model.add(Dense(\n",
    "            units=units,\n",
    "            kernel_regularizer=l2(regularizacao),\n",
    "            name=f'dense_{i + 1}'\n",
    "        ))\n",
    "\n",
    "        # Adicionar batch normalization antes da ativação\n",
    "        if batch_norm:\n",
    "            model.add(BatchNormalization(name=f'batch_norm_{i + 1}'))\n",
    "\n",
    "        # Adicionar função de ativação\n",
    "        if activation == 'leaky_relu':\n",
    "            model.add(LeakyReLU(alpha=0.1, name=f'leaky_relu_{i + 1}'))\n",
    "        elif activation == 'prelu':\n",
    "            model.add(PReLU(name=f'prelu_{i + 1}'))\n",
    "        else:\n",
    "            model.add(Activation(activation, name=f'activation_{i + 1}'))\n",
    "\n",
    "        # Adicionar dropout para regularização\n",
    "        if dropout_rate > 0:\n",
    "            model.add(Dropout(dropout_rate, name=f'dropout_{i + 1}'))\n",
    "\n",
    "    # Camada de saída\n",
    "    model.add(Dense(1, activation='sigmoid', name='output'))\n",
    "\n",
    "    # Compilar modelo\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            'recall'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Resumo do modelo\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def criar_modelo_cnn(input_shape,\n",
    "                     filtros=[64, 32, 16],\n",
    "                     kernel_sizes=[3, 3, 3],\n",
    "                     ativacoes=['relu', 'relu', 'relu'],\n",
    "                     dropout_rates=[0.3, 0.3, 0.3],\n",
    "                     pool_sizes=[2, 2, 2],\n",
    "                     regularizacao=0.001,\n",
    "                     learning_rate=0.001,\n",
    "                     batch_norm=True):\n",
    "    \"\"\"\n",
    "    Cria um modelo CNN 1D para classificação binária de dados tabulares.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Forma dos dados de entrada (deve ser 3D para CNN)\n",
    "        filtros (list): Lista com o número de filtros em cada camada convolucional\n",
    "        kernel_sizes (list): Lista com os tamanhos do kernel para cada camada\n",
    "        ativacoes (list): Lista com as funções de ativação para cada camada\n",
    "        dropout_rates (list): Lista com as taxas de dropout para cada camada\n",
    "        pool_sizes (list): Lista com os tamanhos de pooling para cada camada\n",
    "        regularizacao (float): Coeficiente de regularização L2\n",
    "        learning_rate (float): Taxa de aprendizado\n",
    "        batch_norm (bool): Se True, adiciona camadas de Batch Normalization\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: Modelo CNN compilado\n",
    "    \"\"\"\n",
    "    logger.info(f\"Criando modelo CNN com {len(filtros)} camadas convolucionais\")\n",
    "    logger.info(f\"Filtros: {filtros}\")\n",
    "    logger.info(f\"Kernel sizes: {kernel_sizes}\")\n",
    "    logger.info(f\"Ativações: {ativacoes}\")\n",
    "\n",
    "    # Verificar se os parâmetros têm o mesmo comprimento\n",
    "    assert len(filtros) == len(kernel_sizes) == len(ativacoes) == len(dropout_rates) == len(pool_sizes), \\\n",
    "        \"filtros, kernel_sizes, ativacoes, dropout_rates e pool_sizes devem ter o mesmo comprimento\"\n",
    "\n",
    "    # Verificar se a forma de entrada é 3D\n",
    "    if len(input_shape) != 2:\n",
    "        logger.warning(f\"Input shape {input_shape} não é 3D. Adicionando dimensão extra.\")\n",
    "        input_shape = (*input_shape, 1)\n",
    "\n",
    "    # Criar modelo\n",
    "    model = Sequential()\n",
    "\n",
    "    # Camada de entrada\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    # Camadas convolucionais\n",
    "    for i, (filters, kernel_size, activation, dropout_rate, pool_size) in enumerate(\n",
    "            zip(filtros, kernel_sizes, ativacoes, dropout_rates, pool_sizes)):\n",
    "\n",
    "        # Adicionar camada convolucional com regularização L2\n",
    "        model.add(Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            padding='same',\n",
    "            kernel_regularizer=l2(regularizacao),\n",
    "            name=f'conv_{i + 1}'\n",
    "        ))\n",
    "\n",
    "        # Adicionar batch normalization antes da ativação\n",
    "        if batch_norm:\n",
    "            model.add(BatchNormalization(name=f'batch_norm_{i + 1}'))\n",
    "\n",
    "        # Adicionar função de ativação\n",
    "        if activation == 'leaky_relu':\n",
    "            model.add(LeakyReLU(alpha=0.1, name=f'leaky_relu_{i + 1}'))\n",
    "        elif activation == 'prelu':\n",
    "            model.add(PReLU(name=f'prelu_{i + 1}'))\n",
    "        else:\n",
    "            model.add(Activation(activation, name=f'activation_{i + 1}'))\n",
    "\n",
    "        # Adicionar max pooling\n",
    "        model.add(MaxPooling1D(pool_size=pool_size, padding='same', name=f'pool_{i + 1}'))\n",
    "\n",
    "        # Adicionar dropout para regularização\n",
    "        if dropout_rate > 0:\n",
    "            model.add(Dropout(dropout_rate, name=f'dropout_{i + 1}'))\n",
    "\n",
    "    # Flatten para conectar às camadas densas\n",
    "    model.add(Flatten(name='flatten'))\n",
    "\n",
    "    # Camada densa intermediária\n",
    "    model.add(Dense(32, kernel_regularizer=l2(regularizacao), name='dense_1'))\n",
    "    if batch_norm:\n",
    "        model.add(BatchNormalization(name='batch_norm_dense'))\n",
    "    model.add(Activation('relu', name='activation_dense'))\n",
    "    model.add(Dropout(0.3, name='dropout_dense'))\n",
    "\n",
    "    # Camada de saída\n",
    "    model.add(Dense(1, activation='sigmoid', name='output'))\n",
    "\n",
    "    # Compilar modelo\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            'recall'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Resumo do modelo\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def criar_modelo_hibrido(input_shape,\n",
    "                         filtros_cnn=[64, 32],\n",
    "                         kernel_sizes=[3, 3],\n",
    "                         unidades_lstm=32,\n",
    "                         unidades_densas=[64, 32],\n",
    "                         ativacoes=['relu', 'relu'],\n",
    "                         dropout_rates=[0.3, 0.3],\n",
    "                         regularizacao=0.001,\n",
    "                         learning_rate=0.001,\n",
    "                         batch_norm=True):\n",
    "    \"\"\"\n",
    "    Cria um modelo híbrido CNN-LSTM para classificação binária de dados tabulares.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Forma dos dados de entrada (deve ser 3D)\n",
    "        filtros_cnn (list): Lista com o número de filtros em cada camada convolucional\n",
    "        kernel_sizes (list): Lista com os tamanhos do kernel para cada camada\n",
    "        unidades_lstm (int): Número de unidades na camada LSTM\n",
    "        unidades_densas (list): Lista com o número de neurônios em cada camada densa\n",
    "        ativacoes (list): Lista com as funções de ativação para cada camada densa\n",
    "        dropout_rates (list): Lista com as taxas de dropout para cada camada densa\n",
    "        regularizacao (float): Coeficiente de regularização L2\n",
    "        learning_rate (float): Taxa de aprendizado\n",
    "        batch_norm (bool): Se True, adiciona camadas de Batch Normalization\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: Modelo híbrido CNN-LSTM compilado\n",
    "    \"\"\"\n",
    "    logger.info(\"Criando modelo híbrido CNN-LSTM\")\n",
    "    logger.info(f\"Filtros CNN: {filtros_cnn}\")\n",
    "    logger.info(f\"Unidades LSTM: {unidades_lstm}\")\n",
    "    logger.info(f\"Unidades densas: {unidades_densas}\")\n",
    "\n",
    "    # Verificar se a forma de entrada é 3D\n",
    "    if len(input_shape) != 2:\n",
    "        logger.warning(f\"Input shape {input_shape} não é 3D. Adicionando dimensão extra.\")\n",
    "        input_shape = (*input_shape, 1)\n",
    "\n",
    "    # Verificar se os parâmetros têm o mesmo comprimento\n",
    "    assert len(filtros_cnn) == len(kernel_sizes), \\\n",
    "        \"filtros_cnn e kernel_sizes devem ter o mesmo comprimento\"\n",
    "    assert len(unidades_densas) == len(ativacoes) == len(dropout_rates), \\\n",
    "        \"unidades_densas, ativacoes e dropout_rates devem ter o mesmo comprimento\"\n",
    "\n",
    "    # Criar modelo usando a API funcional\n",
    "    inputs = Input(shape=input_shape\n",
    "                   )\n",
    "\n",
    "    # Camadas convolucionais\n",
    "    x = inputs\n",
    "    for i, (filters, kernel_size) in enumerate(zip(filtros_cnn, kernel_sizes)):\n",
    "        x = Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            padding='same',\n",
    "            kernel_regularizer=l2(regularizacao),\n",
    "            name=f'conv_{i + 1}'\n",
    "        )(x)\n",
    "\n",
    "        if batch_norm:\n",
    "            x = BatchNormalization(name=f'batch_norm_conv_{i + 1}')(x)\n",
    "\n",
    "        x = Activation('relu', name=f'activation_conv_{i + 1}')(x)\n",
    "        x = MaxPooling1D(pool_size=2, padding='same', name=f'pool_{i + 1}')(x)\n",
    "\n",
    "    # Camada LSTM bidirecional\n",
    "    x = Bidirectional(LSTM(unidades_lstm, return_sequences=True, name='lstm_1'))(x)\n",
    "    x = Dropout(0.3, name='dropout_lstm')(x)\n",
    "\n",
    "    # Global Average Pooling\n",
    "    x = GlobalAveragePooling1D(name='global_avg_pool')(x)\n",
    "\n",
    "    # Camadas densas\n",
    "    for i, (units, activation, dropout_rate) in enumerate(zip(unidades_densas, ativacoes, dropout_rates)):\n",
    "        x = Dense(units, kernel_regularizer=l2(regularizacao), name=f'dense_{i + 1}')(x)\n",
    "\n",
    "        if batch_norm:\n",
    "            x = BatchNormalization(name=f'batch_norm_dense_{i + 1}')(x)\n",
    "\n",
    "        if activation == 'leaky_relu':\n",
    "            x = LeakyReLU(alpha=0.1, name=f'leaky_relu_{i + 1}')(x)\n",
    "        elif activation == 'prelu':\n",
    "            x = PReLU(name=f'prelu_{i + 1}')(x)\n",
    "        else:\n",
    "            x = Activation(activation, name=f'activation_dense_{i + 1}')(x)\n",
    "\n",
    "        if dropout_rate > 0:\n",
    "            x = Dropout(dropout_rate, name=f'dropout_dense_{i + 1}')(x)\n",
    "\n",
    "    # Camada de saída\n",
    "    outputs = Dense(1, activation='sigmoid', name='output')(x)\n",
    "\n",
    "    # Criar e compilar modelo\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            'recall'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Resumo do modelo\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def criar_callbacks(nome_modelo, paciencia=10, min_delta=0.001, fator_reducao=0.5, min_lr=1e-6):\n",
    "    \"\"\"\n",
    "    Cria callbacks para treinamento do modelo.\n",
    "\n",
    "    Args:\n",
    "        nome_modelo (str): Nome do modelo para salvar\n",
    "        paciencia (int): Número de épocas para esperar antes de parar o treinamento\n",
    "        min_delta (float): Mínima mudança para considerar como melhoria\n",
    "        fator_reducao (float): Fator para reduzir a taxa de aprendizado\n",
    "        min_lr (float): Taxa de aprendizado mínima\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de callbacks\n",
    "    \"\"\"\n",
    "    # Criar diretório para logs do TensorBoard\n",
    "    log_dir = f\"{RESULTS_DIR}/logs/{nome_modelo}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        # Early stopping para evitar overfitting\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=paciencia,\n",
    "            min_delta=min_delta,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "\n",
    "        # Model checkpoint para salvar o melhor modelo\n",
    "        ModelCheckpoint(\n",
    "            filepath=f\"{RESULTS_DIR}/modelos/{nome_modelo}_best.h5\",\n",
    "            monitor='val_auc',\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "\n",
    "        # Reduce learning rate quando o treinamento estagnar\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=fator_reducao,\n",
    "            patience=paciencia // 2,\n",
    "            min_lr=min_lr,\n",
    "            verbose=1\n",
    "        ),\n",
    "\n",
    "        # TensorBoard para visualização do treinamento\n",
    "        TensorBoard(\n",
    "            log_dir=log_dir,\n",
    "            histogram_freq=1,\n",
    "            write_graph=True,\n",
    "            update_freq='epoch'\n",
    "        ),\n",
    "\n",
    "        # CSV Logger para salvar histórico de treinamento\n",
    "        CSVLogger(\n",
    "            filename=f\"{RESULTS_DIR}/logs/{nome_modelo}_history.csv\",\n",
    "            separator=',',\n",
    "            append=False\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return callbacks"
   ],
   "id": "d4f2ceb2634d7b79",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T14:41:11.653411Z",
     "start_time": "2025-07-01T14:41:11.630411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =====================================================================\n",
    "# FUNÇÕES PARA COMPARAÇÃO DE MODELOS\n",
    "# =====================================================================\n",
    "\n",
    "def treinar_modelos_classicos(X_train, y_train, X_test, y_test, feature_names):\n",
    "    \"\"\"\n",
    "    Treina e avalia modelos clássicos de machine learning.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): Dados de treinamento\n",
    "        y_train (np.ndarray): Rótulos de treinamento\n",
    "        X_test (np.ndarray): Dados de teste\n",
    "        y_test (np.ndarray): Rótulos de teste\n",
    "        feature_names (list): Nomes das features\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame com resultados dos modelos\n",
    "    \"\"\"\n",
    "    logger.info(\"Treinando modelos clássicos de machine learning\")\n",
    "\n",
    "    # Garantir que X_train e X_test sejam DataFrames com nomes de features\n",
    "    if not isinstance(X_train, pd.DataFrame):\n",
    "        X_train = pd.DataFrame(X_train, columns=feature_names)\n",
    "    if not isinstance(X_test, pd.DataFrame):\n",
    "        X_test = pd.DataFrame(X_test, columns=feature_names)\n",
    "\n",
    "    # Definir modelos\n",
    "    modelos = {\n",
    "        'Regressão Logística': LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced', verbose=1),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', verbose=1),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42, verbose=1),\n",
    "        'XGBoost': XGBClassifier(eval_metric='logloss', use_label_encoder=False, random_state=42, verbose=1),\n",
    "        'LightGBM': LGBMClassifier(random_state=42, verbose=1),\n",
    "        'SVM': SVC(kernel='rbf', probability=True, random_state=42, class_weight='balanced', verbose=1),\n",
    "        'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n",
    "        'Naive Bayes': GaussianNB(),\n",
    "    }\n",
    "\n",
    "    # Resultados\n",
    "    resultados = []\n",
    "\n",
    "    # Treinar e avaliar cada modelo\n",
    "    for nome, modelo in modelos.items():\n",
    "        logger.info(f\"Treinando modelo: {nome}\")\n",
    "\n",
    "        # Treinar modelo\n",
    "        modelo.fit(X_train, y_train)\n",
    "\n",
    "        # Fazer predições\n",
    "        y_pred = modelo.predict(X_test)\n",
    "\n",
    "        # Obter probabilidades\n",
    "        if hasattr(modelo, \"predict_proba\"):\n",
    "            y_prob = modelo.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            # Para SVM sem probabilidades\n",
    "            y_prob = modelo.decision_function(X_test)\n",
    "            # Normalizar para [0, 1]\n",
    "            y_prob = (y_prob - y_prob.min()) / (y_prob.max() - y_prob.min())\n",
    "\n",
    "        # Calcular métricas\n",
    "        metrics = {\n",
    "            'modelo': nome,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_prob),\n",
    "            'average_precision': average_precision_score(y_test, y_prob)\n",
    "        }\n",
    "\n",
    "        resultados.append(metrics)\n",
    "\n",
    "        # Salvar modelo\n",
    "        with open(f\"{RESULTS_DIR}/modelos/modelo_{nome.replace(' ', '_').lower()}.pkl\", 'wb') as f:\n",
    "            pickle.dump(modelo, f)\n",
    "\n",
    "        # Visualizar matriz de confusão\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "        plt.title(f'Matriz de Confusão - {nome}', fontsize=15)\n",
    "        plt.ylabel('Valor Real', fontsize=12)\n",
    "        plt.xlabel('Valor Predito', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{RESULTS_DIR}/graficos/confusao/matriz_confusao_{nome.replace(' ', '_').lower()}.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # Visualizar curva ROC\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Taxa de Falsos Positivos', fontsize=12)\n",
    "        plt.ylabel('Taxa de Verdadeiros Positivos', fontsize=12)\n",
    "        plt.title(f'Curva ROC - {nome}', fontsize=15)\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{RESULTS_DIR}/graficos/roc/curva_roc_{nome.replace(' ', '_').lower()}.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # Para Random Forest, visualizar importância das features\n",
    "        if nome == 'Random Forest':\n",
    "            importances = modelo.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1]\n",
    "\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.bar(range(len(indices[:15])), importances[indices[:15]], align='center')\n",
    "            plt.xticks(range(len(indices[:15])), [feature_names[i] for i in indices[:15]], rotation=90)\n",
    "            plt.title('Importância das Features - Random Forest', fontsize=15)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{RESULTS_DIR}/graficos/importancia/importancia_features_random_forest.png\", dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "    # Criar DataFrame com resultados\n",
    "    resultados_df = pd.DataFrame(resultados)\n",
    "\n",
    "    # Salvar resultados\n",
    "    resultados_df.to_csv(f\"{RESULTS_DIR}/resultados_modelos_classicos.csv\", index=False)\n",
    "\n",
    "    return resultados_df\n",
    "\n",
    "\n",
    "def comparar_todos_modelos(resultados_classicos, resultados_redes):\n",
    "    \"\"\"\n",
    "    Compara todos os modelos treinados.\n",
    "\n",
    "    Args:\n",
    "        resultados_classicos (pd.DataFrame): Resultados dos modelos clássicos\n",
    "        resultados_redes (pd.DataFrame): Resultados das redes neurais\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame com resultados de todos os modelos\n",
    "    \"\"\"\n",
    "    logger.info(\"Comparando todos os modelos\")\n",
    "\n",
    "    # Combinar resultados\n",
    "    todos_resultados = pd.concat([resultados_classicos, resultados_redes], ignore_index=True)\n",
    "\n",
    "    # Ordenar por F1-score\n",
    "    todos_resultados_sorted = todos_resultados.sort_values('f1', ascending=False)\n",
    "\n",
    "    # Salvar resultados\n",
    "    todos_resultados_sorted.to_csv(f\"{RESULTS_DIR}/resultados_todos_modelos.csv\", index=False)\n",
    "\n",
    "    # Métricas para visualização\n",
    "    metricas = ['accuracy', 'balanced_accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "    # Visualizar comparação de métricas\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    for i, metrica in enumerate(metricas, 1):\n",
    "        plt.subplot(2, 3, i)\n",
    "        sns.barplot(x='modelo', y=metrica, data=todos_resultados_sorted, palette='viridis')\n",
    "        plt.title(f'{metrica.upper()}', fontsize=15)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle('Comparação de Métricas entre Modelos', fontsize=20, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/comparacao_metricas_todos_modelos.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Gráfico de radar\n",
    "    # Selecionar top 5 modelos\n",
    "    top_modelos = todos_resultados_sorted.head(5)\n",
    "\n",
    "    # Preparar dados para gráfico de radar\n",
    "    categories = metricas\n",
    "    N = len(categories)\n",
    "\n",
    "    # Criar ângulos para o gráfico de radar\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Fechar o círculo\n",
    "\n",
    "    # Criar figura\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "\n",
    "    # Adicionar linhas de grade\n",
    "    plt.xticks(angles[:-1], categories, size=12)\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks([0.2, 0.4, 0.6, 0.8, 1.0], [\"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"], size=10)\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    # Plotar cada modelo\n",
    "    for i, row in top_modelos.iterrows():\n",
    "        values = row[metricas].values.tolist()\n",
    "        values += values[:1]  # Fechar o círculo\n",
    "        ax.plot(angles, values, linewidth=2, linestyle='solid', label=row['modelo'])\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "\n",
    "    # Adicionar legenda\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "\n",
    "    plt.title('Comparação dos Top 5 Modelos', size=20, y=1.1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/radar_top_modelos.png\", dpi=300)\n",
    "    plt.close()"
   ],
   "id": "a32b20b45289c00b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T14:41:17.072843Z",
     "start_time": "2025-07-01T14:41:17.044835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =====================================================================\n",
    "# FUNÇÕES PARA TREINAMENTO E AVALIAÇÃO DE MODELOS\n",
    "# =====================================================================\n",
    "\n",
    "def treinar_modelo(modelo, X_train, y_train, X_val, y_val,\n",
    "                   nome_modelo, batch_size=32, epochs=100,\n",
    "                   class_weight=None, verbose=1):\n",
    "    \"\"\"\n",
    "    Treina o modelo de rede neural.\n",
    "\n",
    "    Args:\n",
    "        modelo (tf.keras.Model): Modelo a ser treinado\n",
    "        X_train (np.ndarray): Dados de treinamento\n",
    "        y_train (np.ndarray): Rótulos de treinamento\n",
    "        X_val (np.ndarray): Dados de validação\n",
    "        y_val (np.ndarray): Rótulos de validação\n",
    "        nome_modelo (str): Nome do modelo para salvar\n",
    "        batch_size (int): Tamanho do batch\n",
    "        epochs (int): Número máximo de épocas\n",
    "        class_weight (dict): Pesos para as classes\n",
    "        verbose (int): Nível de verbosidade\n",
    "\n",
    "    Returns:\n",
    "        tuple: (modelo treinado, histórico de treinamento)\n",
    "    \"\"\"\n",
    "    logger.info(f\"Iniciando treinamento do modelo {nome_modelo}\")\n",
    "    logger.info(f\"Batch size: {batch_size}, Épocas: {epochs}\")\n",
    "\n",
    "    # Verificar se é necessário reshape para CNN ou modelos híbridos\n",
    "    if 'cnn' in nome_modelo.lower() or 'hibrido' in nome_modelo.lower():\n",
    "        if len(X_train.shape) == 2:\n",
    "            logger.info(\"Adicionando dimensão extra para dados CNN/híbridos\")\n",
    "            X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "            X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
    "\n",
    "    # Calcular class weights se não fornecidos\n",
    "    if class_weight is None and len(np.unique(y_train)) == 2:\n",
    "        logger.info(\"Calculando pesos de classe para balanceamento\")\n",
    "        n_samples = len(y_train)\n",
    "        n_positives = np.sum(y_train)\n",
    "        n_negatives = n_samples - n_positives\n",
    "\n",
    "        weight_for_0 = (1 / n_negatives) * (n_samples / 2.0)\n",
    "        weight_for_1 = (1 / n_positives) * (n_samples / 2.0)\n",
    "\n",
    "        class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "        logger.info(f\"Class weights: {class_weight}\")\n",
    "\n",
    "    # Criar callbacks\n",
    "    callbacks = criar_callbacks(nome_modelo)\n",
    "\n",
    "    # Registrar tempo de início\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Treinar o modelo\n",
    "    history = modelo.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        class_weight=class_weight,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Calcular tempo de treinamento\n",
    "    training_time = time.time() - start_time\n",
    "    logger.info(f\"Treinamento concluído em {training_time:.2f} segundos\")\n",
    "\n",
    "    # Carregar o melhor modelo\n",
    "    try:\n",
    "        logger.info(f\"Carregando o melhor modelo de {RESULTS_DIR}/modelos/{nome_modelo}_best.h5\")\n",
    "        modelo = load_model(f\"{RESULTS_DIR}/modelos/{nome_modelo}_best.h5\")\n",
    "    except:\n",
    "        logger.warning(\"Não foi possível carregar o melhor modelo. Usando o modelo atual.\")\n",
    "\n",
    "    # Salvar o modelo final\n",
    "    modelo.save(f\"{RESULTS_DIR}/modelos/{nome_modelo}_final.h5\")\n",
    "    logger.info(f\"Modelo final salvo em {RESULTS_DIR}/modelos/{nome_modelo}_final.h5\")\n",
    "\n",
    "    # Salvar histórico de treinamento\n",
    "    hist_df = pd.DataFrame(history.history)\n",
    "    hist_df.to_csv(f\"{RESULTS_DIR}/logs/{nome_modelo}_history_full.csv\", index=False)\n",
    "\n",
    "\n",
    "def avaliar_modelo(modelo, X_test, y_test, nome_modelo, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Avalia o desempenho do modelo nos dados de teste.\n",
    "\n",
    "    Args:\n",
    "        modelo (tf.keras.Model): Modelo treinado\n",
    "        X_test (np.ndarray): Dados de teste\n",
    "        y_test (np.ndarray): Rótulos de teste\n",
    "        nome_modelo (str): Nome do modelo para salvar resultados\n",
    "        threshold (float): Limiar para classificação binária\n",
    "\n",
    "    Returns:\n",
    "        dict: Dicionário com métricas de desempenho\n",
    "    \"\"\"\n",
    "    logger.info(f\"Avaliando modelo {nome_modelo} nos dados de teste\")\n",
    "\n",
    "    # Verificar se é necessário reshape para CNN ou modelos híbridos\n",
    "    if 'cnn' in nome_modelo.lower() or 'hibrido' in nome_modelo.lower():\n",
    "        if len(X_test.shape) == 2:\n",
    "            logger.info(\"Adicionando dimensão extra para dados CNN/híbridos\")\n",
    "            X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "    # Fazer predições\n",
    "    y_pred_prob = modelo.predict(X_test)\n",
    "    y_pred = (y_pred_prob > threshold).astype(int).flatten()\n",
    "    y_pred_prob = y_pred_prob.flatten()\n",
    "\n",
    "    # Calcular métricas\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_prob),\n",
    "        'average_precision': average_precision_score(y_test, y_pred_prob)\n",
    "    }\n",
    "\n",
    "    # Exibir métricas\n",
    "    logger.info(\"Métricas de desempenho:\")\n",
    "    for metric, value in metrics.items():\n",
    "        logger.info(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    # Salvar métricas em CSV\n",
    "    pd.DataFrame([metrics]).to_csv(f\"{RESULTS_DIR}/{nome_modelo}_metricas.csv\", index=False)\n",
    "\n",
    "    # Matriz de confusão\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Relatório de classificação\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    pd.DataFrame(report).transpose().to_csv(f\"{RESULTS_DIR}/{nome_modelo}_classification_report.csv\")\n",
    "\n",
    "    # Visualizações\n",
    "    visualizar_resultados(y_test, y_pred, y_pred_prob, nome_modelo)\n",
    "\n",
    "    return metrics, y_pred, y_pred_prob\n",
    "\n",
    "\n",
    "def visualizar_resultados(y_true, y_pred, y_prob, nome_modelo):\n",
    "    \"\"\"\n",
    "    Cria visualizações para os resultados do modelo.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): Rótulos verdadeiros\n",
    "        y_pred (np.ndarray): Predições binárias\n",
    "        y_prob (np.ndarray): Probabilidades preditas\n",
    "        nome_modelo (str): Nome do modelo para salvar visualizações\n",
    "    \"\"\"\n",
    "    logger.info(f\"Gerando visualizações para resultados do modelo {nome_modelo}\")\n",
    "\n",
    "    # 1. Matriz de confusão\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Normalizar matriz de confusão\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Plotar matriz de confusão\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title(f'Matriz de Confusão - {nome_modelo}', fontsize=15)\n",
    "    plt.ylabel('Valor Real', fontsize=12)\n",
    "    plt.xlabel('Valor Predito', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/{nome_modelo}_matriz_confusao.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Matriz de confusão normalizada\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', cbar=False)\n",
    "    plt.title(f'Matriz de Confusão Normalizada - {nome_modelo}', fontsize=15)\n",
    "    plt.ylabel('Valor Real', fontsize=12)\n",
    "    plt.xlabel('Valor Predito', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/{nome_modelo}_matriz_confusao_norm.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Curva ROC\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Taxa de Falsos Positivos', fontsize=12)\n",
    "    plt.ylabel('Taxa de Verdadeiros Positivos', fontsize=12)\n",
    "    plt.title(f'Curva ROC - {nome_modelo}', fontsize=15)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/{nome_modelo}_curva_roc.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 3. Curva Precision-Recall\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "    avg_precision = average_precision_score(y_true, y_prob)\n",
    "\n",
    "    plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AP = {avg_precision:.4f})')\n",
    "    plt.axhline(y=sum(y_true) / len(y_true), color='red', linestyle='--', label='Baseline')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall', fontsize=12)\n",
    "    plt.ylabel('Precision', fontsize=12)\n",
    "    plt.title(f'Curva Precision-Recall - {nome_modelo}', fontsize=15)\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/{nome_modelo}_curva_precision_recall.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 4. Histograma de probabilidades\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Separar probabilidades por classe\n",
    "    prob_pos = y_prob[y_true == 1]\n",
    "    prob_neg = y_prob[y_true == 0]\n",
    "\n",
    "    plt.hist(prob_pos, bins=20, alpha=0.5, color='green', label='Classe Positiva (Diabetes)')\n",
    "    plt.hist(prob_neg, bins=20, alpha=0.5, color='red', label='Classe Negativa (Não Diabetes)')\n",
    "\n",
    "    plt.axvline(x=0.5, color='black', linestyle='--', label='Limiar (0.5)')\n",
    "    plt.xlabel('Probabilidade Predita', fontsize=12)\n",
    "    plt.ylabel('Contagem', fontsize=12)\n",
    "    plt.title(f'Distribuição de Probabilidades - {nome_modelo}', fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/{nome_modelo}_distribuicao_probabilidades.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def visualizar_historico_treinamento(historico, nome_modelo):\n",
    "    \"\"\"\n",
    "    Visualiza o histórico de treinamento do modelo.\n",
    "\n",
    "    Args:\n",
    "        historico (tf.keras.callbacks.History): Histórico de treinamento\n",
    "        nome_modelo (str): Nome do modelo para salvar visualizações\n",
    "    \"\"\"\n",
    "    logger.info(f\"Visualizando histórico de treinamento do modelo {nome_modelo}\")\n",
    "\n",
    "    # 1. Curvas de perda (loss)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(historico['loss'], label='Treino', color='blue')\n",
    "    plt.plot(historico['val_loss'], label='Validação', color='orange')\n",
    "    plt.title(f'Curvas de Perda - {nome_modelo}', fontsize=15)\n",
    "    plt.xlabel('Época', fontsize=12)\n",
    "    plt.ylabel('Perda (Binary Crossentropy)', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/{nome_modelo}_curvas_perda.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Curvas de acurácia\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(historico['accuracy'], label='Treino', color='blue')\n",
    "    plt.plot(historico['val_accuracy'], label='Validação', color='orange')\n",
    "    plt.title(f'Curvas de Acurácia - {nome_modelo}', fontsize=15)\n",
    "    plt.xlabel('Época', fontsize=12)\n",
    "    plt.ylabel('Acurácia', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/{nome_modelo}_curvas_acuracia.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 3. Curvas de métricas adicionais\n",
    "    if 'precision' in historico.columns and 'recall' in historico.columns and 'auc' in historico.columns:\n",
    "        plt.figure(figsize=(18, 6))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(historico['precision'], label='Treino', color='blue')\n",
    "        plt.plot(historico['val_precision'], label='Validação', color='orange')\n",
    "        plt.title('Precisão', fontsize=13)\n",
    "        plt.xlabel('Época', fontsize=11)\n",
    "        plt.ylabel('Precisão', fontsize=11)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(historico['recall'], label='Treino', color='blue')\n",
    "        plt.plot(historico['val_recall'], label='Validação', color='orange')\n",
    "        plt.title('Recall', fontsize=13)\n",
    "        plt.xlabel('Época', fontsize=11)\n",
    "        plt.ylabel('Recall', fontsize=11)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(historico['auc'], label='Treino', color='blue')\n",
    "        plt.plot(historico['val_auc'], label='Validação', color='orange')\n",
    "        plt.title('AUC', fontsize=13)\n",
    "        plt.xlabel('Época', fontsize=11)\n",
    "        plt.ylabel('AUC', fontsize=11)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.suptitle(f'Métricas de Treinamento - {nome_modelo}', fontsize=16, y=1.05)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{RESULTS_DIR}/graficos/{nome_modelo}_curvas_metricas.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    # 4. Curvas de aprendizado (Learning Curves)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Subplot para loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(historico['loss'], label='Treino', color='blue')\n",
    "    plt.plot(historico['val_loss'], label='Validação', color='orange')\n",
    "    plt.title('Perda (Loss)', fontsize=13)\n",
    "    plt.xlabel('Época', fontsize=11)\n",
    "    plt.ylabel('Perda', fontsize=11)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Subplot para accuracy\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(historico['accuracy'], label='Treino', color='blue')\n",
    "    plt.plot(historico['val_accuracy'], label='Validação', color='orange')\n",
    "    plt.title('Acurácia', fontsize=13)\n",
    "    plt.xlabel('Época', fontsize=11)\n",
    "    plt.ylabel('Acurácia', fontsize=11)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Calcular diferença entre treino e validação\n",
    "    if 'loss' in historico.columns and 'val_loss' in historico.columns:\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(historico['loss'] - historico['val_loss'], label='Diferença', color='green')\n",
    "        plt.axhline(y=0, color='red', linestyle='--')\n",
    "        plt.title('Diferença de Perda (Treino - Validação)', fontsize=13)\n",
    "        plt.xlabel('Época', fontsize=11)\n",
    "        plt.ylabel('Diferença', fontsize=11)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Calcular razão entre treino e validação\n",
    "    if 'accuracy' in historico.columns and 'val_accuracy' in historico.columns:\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(historico['accuracy'] / historico['val_accuracy'], label='Razão', color='purple')\n",
    "        plt.axhline(y=1, color='red', linestyle='--')\n",
    "        plt.title('Razão de Acurácia (Treino / Validação)', fontsize=13)\n",
    "        plt.xlabel('Época', fontsize=11)\n",
    "        plt.ylabel('Razão', fontsize=11)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle(f'Curvas de Aprendizado - {nome_modelo}', fontsize=16, y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/{nome_modelo}_curvas_aprendizado.png\", dpi=300)\n",
    "    plt.close()"
   ],
   "id": "c9e03862ce76a35c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T14:41:25.680671Z",
     "start_time": "2025-07-01T14:41:25.654278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =====================================================================\n",
    "# FUNÇÕES PARA INTERPRETABILIDADE DE MODELOS\n",
    "# =====================================================================\n",
    "\n",
    "def analisar_importancia_variaveis(modelo, X_test, y_test, feature_names, nome_modelo, n_repeats=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Analisa a importância das variáveis no modelo usando cálculo manual de permutation importance com roc_auc.\n",
    "\n",
    "    Args:\n",
    "        modelo (tf.keras.Model): Modelo treinado\n",
    "        X_test (np.ndarray): Dados de teste\n",
    "        y_test (np.ndarray): Rótulos de teste\n",
    "        feature_names (list): Nomes das features\n",
    "        nome_modelo (str): Nome do modelo para salvar resultados\n",
    "        n_repeats (int): Número de repetições para permutação\n",
    "        random_state (int): Semente aleatória\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame com importância das variáveis\n",
    "    \"\"\"\n",
    "    logger.info(f\"Analisando importância das variáveis para o modelo {nome_modelo} (cálculo manual)\")\n",
    "\n",
    "    # Determina se o modelo é CNN ou híbrido com base no nome\n",
    "    is_cnn_or_hybrid = 'cnn' in nome_modelo.lower() or 'hibrido' in nome_modelo.lower()\n",
    "\n",
    "    # Garante que X_test seja 2D para a lógica de permutação\n",
    "    X_test_2d = X_test\n",
    "    if is_cnn_or_hybrid and len(X_test.shape) == 3:\n",
    "        logger.info(\"Convertendo dados 3D para 2D para análise de importância\")\n",
    "        X_test_2d = X_test.reshape(X_test.shape[0], X_test.shape[1])\n",
    "\n",
    "    # Função para obter probabilidades do modelo Keras (considerando reshape)\n",
    "    def get_proba(X_input):\n",
    "        X_input_internal = X_input  # Usa cópia para evitar modificar original\n",
    "        if is_cnn_or_hybrid and len(X_input_internal.shape) == 2:\n",
    "            X_input_internal = X_input_internal.reshape(X_input_internal.shape[0], X_input_internal.shape[1], 1)\n",
    "        return modelo.predict(X_input_internal)[:, 0]  # Retorna prob da classe 1\n",
    "\n",
    "    # Calcula a pontuação base (AUC)\n",
    "    try:\n",
    "        baseline_proba = get_proba(X_test_2d)\n",
    "        baseline_score = roc_auc_score(y_test, baseline_proba)\n",
    "        logger.info(f\"Baseline ROC AUC score: {baseline_score:.4f}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao calcular baseline score para {nome_modelo}: {e}\")\n",
    "        return pd.DataFrame()  # Retorna DataFrame vazio em caso de falha\n",
    "\n",
    "    # Inicializa arrays para armazenar importâncias\n",
    "    importances_mean = np.zeros(X_test_2d.shape[1])\n",
    "    importances_std = np.zeros(X_test_2d.shape[1])\n",
    "    all_perm_scores = [[] for _ in range(X_test_2d.shape[1])]\n",
    "\n",
    "    # Itera sobre cada feature\n",
    "    logger.info(\"Calculando importância por permutação manual (pode levar algum tempo)\")\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    for col_idx in tqdm(range(X_test_2d.shape[1]), desc=f\"Permutando features - {nome_modelo}\"):\n",
    "        original_col = X_test_2d[:, col_idx].copy()\n",
    "        perm_scores = []\n",
    "\n",
    "        for _ in range(n_repeats):\n",
    "            # Permuta a coluna atual\n",
    "            X_test_permuted = X_test_2d.copy()\n",
    "            X_test_permuted[:, col_idx] = rng.permutation(original_col)\n",
    "\n",
    "            # Calcula a pontuação com a coluna permutada\n",
    "            try:\n",
    "                permuted_proba = get_proba(X_test_permuted)\n",
    "                perm_score = roc_auc_score(y_test, permuted_proba)\n",
    "                perm_scores.append(perm_score)\n",
    "                all_perm_scores[col_idx].append(perm_score)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Erro ao calcular score para feature {col_idx} permutada: {e}\")\n",
    "                perm_scores.append(np.nan)  # Adiciona NaN em caso de erro\n",
    "                all_perm_scores[col_idx].append(np.nan)\n",
    "\n",
    "        # Calcula a importância como a diferença para a baseline\n",
    "        valid_perm_scores = [s for s in perm_scores if not np.isnan(s)]\n",
    "        if valid_perm_scores:\n",
    "            importances_mean[col_idx] = baseline_score - np.mean(valid_perm_scores)\n",
    "            importances_std[col_idx] = np.std(valid_perm_scores)\n",
    "        else:\n",
    "            importances_mean[col_idx] = 0  # Define importância como 0 se todos os cálculos falharam\n",
    "            importances_std[col_idx] = 0\n",
    "\n",
    "    # Organizar resultados\n",
    "    importancia = pd.DataFrame({\n",
    "        'Feature': feature_names[:X_test_2d.shape[1]],  # Garantir que o número de features corresponda\n",
    "        'Importância': importances_mean,\n",
    "        'Desvio Padrão': importances_std\n",
    "    })\n",
    "\n",
    "    importancia = importancia.sort_values('Importância', ascending=False)\n",
    "\n",
    "    # Salvar resultados\n",
    "    importancia.to_csv(f\"{RESULTS_DIR}/{nome_modelo}_importancia_variaveis_manual.csv\", index=False)\n",
    "\n",
    "    # Visualizar importância das variáveis (top 15)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    top_features = importancia.head(15)\n",
    "    sns.barplot(x='Importância', y='Feature', data=top_features, palette='viridis')\n",
    "    plt.title(f'Top 15 Variáveis Mais Importantes (Manual) - {nome_modelo}', fontsize=15)\n",
    "    plt.xlabel('Importância (Queda no ROC AUC)', fontsize=12)\n",
    "    plt.ylabel('Variável', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/{nome_modelo}_importancia_variaveis_manual.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Visualizar importância com barras de erro\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    top_features = importancia.head(10)\n",
    "    plt.errorbar(\n",
    "        x=top_features['Importância'],\n",
    "        y=range(len(top_features)),\n",
    "        xerr=top_features['Desvio Padrão'],  # Nota: std aqui é do score permutado, não da importância\n",
    "        fmt='o',\n",
    "        capsize=5,\n",
    "        color='blue'\n",
    "    )\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.title(f'Top 10 Variáveis com Desvio Padrão (Manual) - {nome_modelo}', fontsize=15)\n",
    "    plt.xlabel('Importância (Queda no ROC AUC)', fontsize=12)\n",
    "    plt.ylabel('Variável', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{RESULTS_DIR}/graficos/{nome_modelo}_importancia_variaveis_erro_manual.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    return importancia\n",
    "\n",
    "\n",
    "def analisar_shap_values(modelo, X_test, feature_names, nome_modelo, max_display=10):\n",
    "    \"\"\"\n",
    "    Analisa a importância das variáveis usando SHAP values.\n",
    "\n",
    "    Args:\n",
    "        modelo (tf.keras.Model): Modelo treinado\n",
    "        X_test (np.ndarray): Dados de teste\n",
    "        feature_names (list): Nomes das features\n",
    "        nome_modelo (str): Nome do modelo para salvar resultados\n",
    "        max_display (int): Número máximo de features para exibir\n",
    "\n",
    "    Returns:\n",
    "        tuple: (explainer, shap_values)\n",
    "    \"\"\"\n",
    "    logger.info(f\"Analisando SHAP values para o modelo {nome_modelo}\")\n",
    "\n",
    "    # Verificar se é necessário reshape para CNN ou modelos híbridos\n",
    "    X_test_2d = X_test\n",
    "    if 'cnn' in nome_modelo.lower() or 'hibrido' in nome_modelo.lower():\n",
    "        if len(X_test.shape) == 3:\n",
    "            logger.info(\"Convertendo dados 3D para 2D para análise SHAP\")\n",
    "            X_test_2d = X_test.reshape(X_test.shape[0], X_test.shape[1])\n",
    "\n",
    "    # Função de predição para SHAP\n",
    "    def predict_func(X):\n",
    "        if 'cnn' in nome_modelo.lower() or 'hibrido' in nome_modelo.lower():\n",
    "            X_reshaped = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "            return modelo.predict(X_reshaped)\n",
    "        else:\n",
    "            return modelo.predict(X)\n",
    "\n",
    "    try:\n",
    "        # Usar uma amostra dos dados de teste para eficiência\n",
    "        sample_size = min(100, X_test_2d.shape[0])\n",
    "        X_sample = X_test_2d[:sample_size]\n",
    "\n",
    "        # Criar explainer\n",
    "        logger.info(\"Criando SHAP explainer (KernelExplainer)\")\n",
    "        explainer = shap.KernelExplainer(predict_func, shap.sample(X_sample, 50))\n",
    "\n",
    "        # Calcular SHAP values\n",
    "        logger.info(\"Calculando SHAP values (pode levar algum tempo)\")\n",
    "        shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "        # Resumo das contribuições das features\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        shap.summary_plot(\n",
    "            shap_values,\n",
    "            X_sample,\n",
    "            feature_names=feature_names[:X_test_2d.shape[1]],\n",
    "            max_display=max_display,\n",
    "            show=False\n",
    "        )\n",
    "        plt.title(f'SHAP Summary Plot - {nome_modelo}', fontsize=15)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{RESULTS_DIR}/graficos/shap/{nome_modelo}_shap_summary.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # Gráfico de barras com importância média absoluta\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        shap.summary_plot(\n",
    "            shap_values,\n",
    "            X_sample,\n",
    "            feature_names=feature_names[:X_test_2d.shape[1]],\n",
    "            plot_type='bar',\n",
    "            max_display=max_display,\n",
    "            show=False\n",
    "        )\n",
    "        plt.title(f'SHAP Feature Importance - {nome_modelo}', fontsize=15)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{RESULTS_DIR}/graficos/shap/{nome_modelo}_shap_importance.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # Gráficos de dependência para as top 3 features\n",
    "        shap_df = pd.DataFrame(shap_values, columns=feature_names[:X_test_2d.shape[1]])\n",
    "        mean_abs_shap = np.abs(shap_df).mean().sort_values(ascending=False)\n",
    "        top_features = mean_abs_shap.index[:3].tolist()\n",
    "\n",
    "        for feature in top_features:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            feature_idx = list(feature_names[:X_test_2d.shape[1]]).index(feature)\n",
    "            shap.dependence_plot(\n",
    "                feature_idx,\n",
    "                shap_values,\n",
    "                X_sample,\n",
    "                feature_names=feature_names[:X_test_2d.shape[1]],\n",
    "                show=False\n",
    "            )\n",
    "            plt.title(f'SHAP Dependence Plot - {feature} - {nome_modelo}', fontsize=15)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{RESULTS_DIR}/graficos/shap/{nome_modelo}_shap_dependence_{feature}.png\", dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "        # Gráfico de força para algumas amostras individuais\n",
    "        for i in range(min(3, X_sample.shape[0])):\n",
    "            plt.figure(figsize=(16, 6))\n",
    "            shap.force_plot(\n",
    "                explainer.expected_value,\n",
    "                shap_values[i],\n",
    "                X_sample[i],\n",
    "                feature_names=feature_names[:X_test_2d.shape[1]],\n",
    "                matplotlib=True,\n",
    "                show=False\n",
    "            )\n",
    "            plt.title(f'SHAP Force Plot - Amostra {i + 1} - {nome_modelo}', fontsize=15)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{RESULTS_DIR}/graficos/shap/{nome_modelo}_shap_force_plot_sample_{i + 1}.png\", dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "        return explainer, shap_values\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao calcular SHAP values: {e}\")\n",
    "        logger.info(\"Continuando com outras análises...\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def analisar_lime(modelo, X_train, X_test, y_test, feature_names, nome_modelo, num_amostras=3):\n",
    "    \"\"\"\n",
    "    Analisa o modelo usando LIME (Local Interpretable Model-agnostic Explanations).\n",
    "\n",
    "    Args:\n",
    "        modelo (tf.keras.Model): Modelo treinado\n",
    "        X_train (np.ndarray): Dados de treinamento\n",
    "        X_test (np.ndarray): Dados de teste\n",
    "        y_test (np.ndarray): Rótulos de teste\n",
    "        feature_names (list): Nomes das features\n",
    "        nome_modelo (str): Nome do modelo para salvar resultados\n",
    "        num_amostras (int): Número de amostras para explicar\n",
    "\n",
    "    Returns:\n",
    "        lime.lime_tabular.LimeTabularExplainer: Explainer LIME\n",
    "    \"\"\"\n",
    "    logger.info(f\"Analisando modelo {nome_modelo} usando LIME\")\n",
    "\n",
    "    # Verificar se é necessário reshape para CNN ou modelos híbridos\n",
    "    X_train_2d = X_train\n",
    "    X_test_2d = X_test\n",
    "    if 'cnn' in nome_modelo.lower() or 'hibrido' in nome_modelo.lower():\n",
    "        if len(X_train.shape) == 3:\n",
    "            logger.info(\"Convertendo dados 3D para 2D para análise LIME\")\n",
    "            X_train_2d = X_train.reshape(X_train.shape[0], X_train.shape[1])\n",
    "            X_test_2d = X_test.reshape(X_test.shape[0], X_test.shape[1])\n",
    "\n",
    "    # Função de predição para LIME\n",
    "    def predict_func(X):\n",
    "        if 'cnn' in nome_modelo.lower() or 'hibrido' in nome_modelo.lower():\n",
    "            X_reshaped = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "            return modelo.predict(X_reshaped)\n",
    "        else:\n",
    "            return modelo.predict(X)\n",
    "\n",
    "    try:\n",
    "        # Criar explainer LIME\n",
    "        logger.info(\"Criando LIME explainer\")\n",
    "        explainer = lime_tabular.LimeTabularExplainer(\n",
    "            X_train_2d,\n",
    "            feature_names=feature_names[:X_test_2d.shape[1]],\n",
    "            class_names=['Não Diabetes', 'Diabetes'],\n",
    "            mode='classification'\n",
    "        )\n",
    "\n",
    "        # Selecionar amostras para explicar (incluindo alguns falsos positivos/negativos)\n",
    "        y_pred = (predict_func(X_test_2d) > 0.5).astype(int).flatten()\n",
    "\n",
    "        # Encontrar falsos positivos e falsos negativos\n",
    "        fp_indices = np.where((y_pred == 1) & (y_test == 0))[0]\n",
    "        fn_indices = np.where((y_pred == 0) & (y_test == 1))[0]\n",
    "        tp_indices = np.where((y_pred == 1) & (y_test == 1))[0]\n",
    "        tn_indices = np.where((y_pred == 0) & (y_test == 0))[0]\n",
    "\n",
    "        # Selecionar índices para explicar\n",
    "        indices_to_explain = []\n",
    "\n",
    "        # Adicionar um falso positivo se disponível\n",
    "        if len(fp_indices) > 0:\n",
    "            indices_to_explain.append(('Falso Positivo', fp_indices[0]))\n",
    "\n",
    "        # Adicionar um falso negativo se disponível\n",
    "        if len(fn_indices) > 0:\n",
    "            indices_to_explain.append(('Falso Negativo', fn_indices[0]))\n",
    "\n",
    "        # Adicionar um verdadeiro positivo se disponível\n",
    "        if len(tp_indices) > 0:\n",
    "            indices_to_explain.append(('Verdadeiro Positivo', tp_indices[0]))\n",
    "\n",
    "        # Adicionar um verdadeiro negativo se disponível\n",
    "        if len(tn_indices) > 0:\n",
    "            indices_to_explain.append(('Verdadeiro Negativo', tn_indices[0]))\n",
    "\n",
    "        # Limitar ao número de amostras solicitado\n",
    "        indices_to_explain = indices_to_explain[:num_amostras]\n",
    "\n",
    "        # Gerar explicações\n",
    "        for label, idx in indices_to_explain:\n",
    "            logger.info(f\"Gerando explicação LIME para amostra {idx} ({label})\")\n",
    "\n",
    "            # Gerar explicação\n",
    "            exp = explainer.explain_instance(\n",
    "                X_test_2d[idx],\n",
    "                predict_func,\n",
    "                num_features=10\n",
    "            )\n",
    "\n",
    "            # Visualizar explicação\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            exp.as_pyplot_figure()\n",
    "            plt.title(f'Explicação LIME - {label} (Amostra {idx}) - {nome_modelo}', fontsize=15)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{RESULTS_DIR}/graficos/lime/{nome_modelo}_lime_{label}_amostra_{idx}.png\", dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "        return explainer\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao gerar explicações LIME: {e}\")\n",
    "        logger.info(\"Continuando com outras análises...\")\n",
    "        return None\n"
   ],
   "id": "f8748e0b4e0c49bf",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T14:42:46.306759Z",
     "start_time": "2025-07-01T14:41:33.592570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Carregar e analisar dados\n",
    "df = carregar_dados(\"diabetes_prediction_dataset.csv\")\n",
    "analisar_dados(df)\n",
    "\n",
    "# 2. Pré-processamento dos dados\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, pre_processador, feature_names = preprocessar_dados(\n",
    "    df,\n",
    "    metodo_normalizacao='standard',\n",
    "    metodo_balanceamento='smote',\n",
    "    selecao_features=True,\n",
    "    tratar_outliers=True\n",
    ")\n",
    "\n",
    "# Reshape para CNN\n",
    "X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_val_cnn = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
    "X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n"
   ],
   "id": "7528c487bd514aef",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jabcn\\AppData\\Local\\Temp\\ipykernel_28996\\2326865505.py:159: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  ax = sns.countplot(x='diabetes', data=df, palette=['#3498db', '#e74c3c'])\n",
      "C:\\Users\\jabcn\\AppData\\Local\\Temp\\ipykernel_28996\\2326865505.py:197: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.boxplot(x='diabetes', y=col, data=df, palette=['#3498db', '#e74c3c'])\n",
      "C:\\Users\\jabcn\\AppData\\Local\\Temp\\ipykernel_28996\\2326865505.py:197: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.boxplot(x='diabetes', y=col, data=df, palette=['#3498db', '#e74c3c'])\n",
      "C:\\Users\\jabcn\\AppData\\Local\\Temp\\ipykernel_28996\\2326865505.py:197: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.boxplot(x='diabetes', y=col, data=df, palette=['#3498db', '#e74c3c'])\n",
      "C:\\Users\\jabcn\\AppData\\Local\\Temp\\ipykernel_28996\\2326865505.py:197: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.boxplot(x='diabetes', y=col, data=df, palette=['#3498db', '#e74c3c'])\n",
      "C:\\Users\\jabcn\\OneDrive\\Documents\\Faculdade\\UCP\\python\\.venv\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:111: UserWarning: Features [1 2] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "C:\\Users\\jabcn\\OneDrive\\Documents\\Faculdade\\UCP\\python\\.venv\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-01T14:43:53.367586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Treinar modelos clássicos\n",
    "resultados_classicos = treinar_modelos_classicos(X_train, y_train, X_test, y_test, feature_names)"
   ],
   "id": "d9397016a0ef8788",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    5.4s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2736            8.26s\n",
      "         2           1.1799            7.91s\n",
      "         3           1.1017            7.94s\n",
      "         4           1.0268            8.44s\n",
      "         5           0.9675            8.71s\n",
      "         6           0.9164            8.69s\n",
      "         7           0.8648            8.81s\n",
      "         8           0.8251            8.84s\n",
      "         9           0.7842            8.85s\n",
      "        10           0.7522            8.79s\n",
      "        20           0.5423            8.07s\n",
      "        30           0.4485            7.26s\n",
      "        40           0.3950            6.22s\n",
      "        50           0.3616            5.19s\n",
      "        60           0.3354            4.15s\n",
      "        70           0.3132            3.11s\n",
      "        80           0.2742            2.07s\n",
      "        90           0.2453            1.04s\n",
      "       100           0.2267            0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jabcn\\OneDrive\\Documents\\Faculdade\\UCP\\python\\.venv\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:44:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\", \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 59462, number of negative: 59462\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002182 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1116\n",
      "[LightGBM] [Info] Number of data points in the train set: 118924, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jabcn\\OneDrive\\Documents\\Faculdade\\UCP\\python\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\jabcn\\OneDrive\\Documents\\Faculdade\\UCP\\python\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Treinar e avaliar modelo MLP\n",
    "logger.info(\"Treinando modelo MLP\")\n",
    "modelo_mlp = criar_modelo_mlp(\n",
    "    input_shape=(X_train.shape[1],),\n",
    "    camadas=[128, 64, 32],\n",
    "    ativacoes=['relu', 'relu', 'relu'],\n",
    "    dropout_rates=[0.3, 0.3, 0.3],\n",
    "    regularizacao=0.001,\n",
    "    learning_rate=0.001,\n",
    "    batch_norm=True\n",
    ")\n",
    "\n",
    "treinar_modelo(\n",
    "    modelo_mlp, X_train, y_train, X_val, y_val,\n",
    "    nome_modelo=\"mlp\",\n",
    "    batch_size=128,\n",
    "    epochs=100\n",
    ")"
   ],
   "id": "15f9bfa5a21ca7e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 5. Treinar e avaliar modelo CNN\n",
    "logger.info(\"Treinando modelo CNN\")\n",
    "modelo_cnn = criar_modelo_cnn(\n",
    "    input_shape=(X_train.shape[1], 1),\n",
    "    filtros=[64, 32, 16],\n",
    "    kernel_sizes=[3, 3, 3],\n",
    "    ativacoes=['relu', 'relu', 'relu'],\n",
    "    dropout_rates=[0.3, 0.3, 0.3],\n",
    "    pool_sizes=[2, 2, 2],\n",
    "    regularizacao=0.001,\n",
    "    learning_rate=0.005,\n",
    "    batch_norm=True\n",
    ")\n",
    "\n",
    "treinar_modelo(\n",
    "    modelo_cnn, X_train_cnn, y_train, X_val_cnn, y_val,\n",
    "    nome_modelo=\"cnn\",\n",
    "    batch_size=128,\n",
    "    epochs=100\n",
    ")"
   ],
   "id": "52add393f9d06682",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 6. Treinar e avaliar modelo híbrido\n",
    "logger.info(\"Treinando modelo híbrido CNN-LSTM\")\n",
    "modelo_hibrido = criar_modelo_hibrido(\n",
    "    input_shape=(X_train.shape[1], 1),\n",
    "    filtros_cnn=[64, 32, 16],\n",
    "    kernel_sizes=[3, 3, 3],\n",
    "    unidades_lstm=32,\n",
    "    unidades_densas=[64, 32, 16],\n",
    "    ativacoes=['relu', 'relu', 'relu'],\n",
    "    dropout_rates=[0.3, 0.3, 0.3],\n",
    "    regularizacao=0.001,\n",
    "    learning_rate=0.001,\n",
    "    batch_norm=True\n",
    ")\n",
    "\n",
    "treinar_modelo(\n",
    "    modelo_hibrido, X_train_cnn, y_train, X_val_cnn, y_val,\n",
    "    nome_modelo=\"hibrido\",\n",
    "    batch_size=128,\n",
    "    epochs=100\n",
    ")"
   ],
   "id": "cd6dd53c7f234d0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Caminho do arquivo onde o modelo foi salvo\n",
    "caminho_modelo_mlp = f\"{RESULTS_DIR}/modelos/mlp_final.h5\"\n",
    "caminho_history_mlp = f\"{RESULTS_DIR}/logs/mlp_history.csv\"\n",
    "caminho_modelo_cnn = f\"{RESULTS_DIR}/modelos/cnn_final.h5\"\n",
    "caminho_history_cnn = f\"{RESULTS_DIR}/logs/cnn_history.csv\"\n",
    "caminho_modelo_hibrido = f\"{RESULTS_DIR}/modelos/hibrido_final.h5\"\n",
    "caminho_history_hibrido = f\"{RESULTS_DIR}/logs/hibrido_history.csv\"\n",
    "\n",
    "# Carregar o modelo híbrido\n",
    "modelo_hibrido_treinado = load_model(caminho_modelo_hibrido)\n",
    "historico_hibrido = pd.read_csv(caminho_history_hibrido)\n",
    "\n",
    "# Carregar o modelo MLP\n",
    "modelo_mlp_treinado = load_model(caminho_modelo_mlp)\n",
    "historico_mlp = pd.read_csv(caminho_history_cnn)\n",
    "\n",
    "# Carregar o modelo CNN\n",
    "modelo_cnn_treinado = load_model(caminho_modelo_cnn)\n",
    "historico_cnn = pd.read_csv(caminho_history_mlp)"
   ],
   "id": "11d43641ef5ca4a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Avaliar modelos\n",
    "metricas_mlp, y_pred_mlp, y_prob_mlp = avaliar_modelo(\n",
    "    modelo_mlp_treinado, X_test, y_test, nome_modelo=\"mlp\"\n",
    ")\n",
    "\n",
    "visualizar_historico_treinamento(historico_mlp, \"mlp\")\n",
    "\n",
    "metricas_hibrido, y_pred_hibrido, y_prob_hibrido = avaliar_modelo(\n",
    "    modelo_hibrido_treinado, X_test_cnn, y_test, nome_modelo=\"hibrido\"\n",
    ")\n",
    "\n",
    "visualizar_historico_treinamento(historico_hibrido, \"hibrido\")\n",
    "\n",
    "metricas_cnn, y_pred_cnn, y_prob_cnn = avaliar_modelo(\n",
    "    modelo_cnn_treinado, X_test_cnn, y_test, nome_modelo=\"cnn\"\n",
    ")\n",
    "\n",
    "visualizar_historico_treinamento(historico_cnn, \"cnn\")"
   ],
   "id": "11263b6d85abd65a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# class KerasClassifierWrapper(BaseEstimator, ClassifierMixin):\n",
    "#     \"\"\"\n",
    "#     Wrapper para modelos Keras para torná-los compatíveis com a API do scikit-learn,\n",
    "#     especialmente para funções como permutation_importance que esperam métodos\n",
    "#     predict_proba específicos.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, model, is_cnn_or_hybrid=False):\n",
    "#         self.model = model\n",
    "#         self.is_cnn_or_hybrid = is_cnn_or_hybrid\n",
    "#         # Adiciona o atributo classes_ necessário para compatibilidade\n",
    "#         self.classes_ = np.array([0, 1])\n",
    "#\n",
    "#     def fit(self, X, y):\n",
    "#         # O método fit não é necessário para permutation_importance pois o modelo já está treinado,\n",
    "#         # mas é exigido pela classe BaseEstimator do scikit-learn.\n",
    "#         # Apenas retorna self para manter a compatibilidade.\n",
    "#         return self\n",
    "#\n",
    "#     def predict(self, X):\n",
    "#         # Realiza o reshape se for um modelo CNN/híbrido e a entrada for 2D\n",
    "#         if self.is_cnn_or_hybrid and len(X.shape) == 2:\n",
    "#             X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "#         y_prob = self.model.predict(X)\n",
    "#         # Retorna a classe predita com base no limiar 0.5\n",
    "#         return (y_prob > 0.5).astype(int)\n",
    "#\n",
    "#     def predict_proba(self, X):\n",
    "#         # Realiza o reshape se for um modelo CNN/híbrido e a entrada for 2D\n",
    "#         if self.is_cnn_or_hybrid and len(X.shape) == 2:\n",
    "#             X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "#         y_prob = self.model.predict(X)\n",
    "#         # Retorna as probabilidades para ambas as classes [prob_classe_0, prob_classe_1]\n",
    "#         conforme esperado pelo scikit-learn\n",
    "#         return np.hstack([1 - y_prob, y_prob])\n",
    "#\n",
    "#     # Adiciona a propriedade _estimator_type para identificar como classificador\n",
    "#     @property\n",
    "#     def _estimator_type(self):\n",
    "#         return \"classifier\"\n"
   ],
   "id": "5e18ab82569755f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 7. Análise de importância de variáveis\n",
    "\n",
    "importancia_mlp = analisar_importancia_variaveis(\n",
    "    modelo_mlp_treinado, X_test, y_test, feature_names, \"mlp\"\n",
    ")\n",
    "\n",
    "importancia_cnn = analisar_importancia_variaveis(\n",
    "    modelo_cnn_treinado, X_test_cnn, y_test, feature_names, \"cnn\"\n",
    ")\n",
    "\n",
    "importancia_hibrido = analisar_importancia_variaveis(\n",
    "    modelo_hibrido_treinado, X_test_cnn, y_test, feature_names, \"hibrido\"\n",
    ")\n",
    "\n",
    "# 8. Análise SHAP\n",
    "explainer_mlp, shap_values_mlp = analisar_shap_values(\n",
    "    modelo_mlp_treinado, X_test, feature_names, \"mlp\"\n",
    ")\n",
    "\n",
    "explainer_cnn, shap_values_cnn = analisar_shap_values(\n",
    "    modelo_cnn_treinado, X_test_cnn, feature_names, \"cnn\"\n",
    ")\n",
    "\n",
    "# 9. Análise LIME\n",
    "lime_explainer_mlp = analisar_lime(\n",
    "    modelo_mlp_treinado, X_train, X_test, y_test, feature_names, \"mlp\"\n",
    ")\n"
   ],
   "id": "490ce0913a1cc511",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "caminho_resultados_classicos = f\"{RESULTS_DIR}/resultados_classicos.csv\"\n",
    "resultados_classicos = pd.read_csv(caminho_history_mlp)\n",
    "\n",
    "# 10. Comparação de todos os modelos\n",
    "resultados_redes = pd.DataFrame([\n",
    "    {\n",
    "        'modelo': 'MLP',\n",
    "        'accuracy': metricas_mlp['accuracy'],\n",
    "        'balanced_accuracy': metricas_mlp['balanced_accuracy'],\n",
    "        'precision': metricas_mlp['precision'],\n",
    "        'recall': metricas_mlp['recall'],\n",
    "        'f1': metricas_mlp['f1'],\n",
    "        'roc_auc': metricas_mlp['roc_auc'],\n",
    "        'average_precision': metricas_mlp['average_precision']\n",
    "    },\n",
    "    {\n",
    "        'modelo': 'CNN',\n",
    "        'accuracy': metricas_cnn['accuracy'],\n",
    "        'balanced_accuracy': metricas_cnn['balanced_accuracy'],\n",
    "        'precision': metricas_cnn['precision'],\n",
    "        'recall': metricas_cnn['recall'],\n",
    "        'f1': metricas_cnn['f1'],\n",
    "        'roc_auc': metricas_cnn['roc_auc'],\n",
    "        'average_precision': metricas_cnn['average_precision']\n",
    "    },\n",
    "    {\n",
    "        'modelo': 'Híbrido CNN-LSTM',\n",
    "        'accuracy': metricas_hibrido['accuracy'],\n",
    "        'balanced_accuracy': metricas_hibrido['balanced_accuracy'],\n",
    "        'precision': metricas_hibrido['precision'],\n",
    "        'recall': metricas_hibrido['recall'],\n",
    "        'f1': metricas_hibrido['f1'],\n",
    "        'roc_auc': metricas_hibrido['roc_auc'],\n",
    "        'average_precision': metricas_hibrido['average_precision']\n",
    "    }\n",
    "])\n",
    "\n",
    "comparar_todos_modelos(resultados_classicos, resultados_redes)\n",
    "\n",
    "logger.info(\"Análise completa! Todos os resultados e visualizações foram salvos.\")\n",
    "\n"
   ],
   "id": "f3208cbf0e53725b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
